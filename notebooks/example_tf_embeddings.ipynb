{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22ea4f7-3b6b-4554-aeda-ae85e7f96f0a",
   "metadata": {},
   "source": [
    "# Work on embeddings\n",
    "\n",
    "Objective: Take Podcast Descriptions and create embeddings from them.  Create an application that allows a user to enter their own text and use cosine similarity to find the most similar show descriptions (or episodes based on episode description).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bcf087d1-9581-4d00-89e4-11765da5cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ea783ca8-cf7b-44f8-9908-f1c9eb5da55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90706, 18)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../metadata_with_episode_dates_and_category.tsv',sep='\\t')\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], format='%Y-%m-%d').reset_index(drop=True)\n",
    "df = df[~df['release_date'].isna()]\n",
    "df = df[~df['category'].isna()]\n",
    "df = df[~df['show_description'].isna()]\n",
    "df = df[~df['show_name'].isna()]\n",
    "df = df[~df['episode_description'].isna()]\n",
    "df = df[~df['episode_name'].isna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4599085b-7d8b-4677-ad3b-79cca4c5cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e88f36ae-6d09-4c25-adce-d0a3c97ad7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shows: 15857, Episodes: 90871\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shows: {len(show_descriptions)}, Episodes: {len(episode_descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5dc47852-b743-48d4-8e03-eecc2c71dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = TFAutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909a82a-209d-47e9-af7e-bba9e7ad3836",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Example from Huggingface\n",
    "https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "47cdf890-cc9f-47f6-9852-fddad6d7b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.92, Query: Around 9 Million people live in London.\n",
      "Score: 0.49, Query: London is known for its financial district.\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)\n",
    "doc_emb = encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query_emb @ tf.transpose(doc_emb))[0].numpy().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(f\"Score: {score:.2f}, Query: {doc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ae4b7-20b2-4af6-8869-8327ceb7c495",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13beeb-a70e-4791-a9d5-e8df4a8ce7a5",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Performing Comparisons\n",
    "\n",
    "Benchmark embeddings comparison for all Show Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cf2e6f10-574b-41b1-b271-31895d07d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35s to compare a worst cast dot product with 100000 rows.\n"
     ]
    }
   ],
   "source": [
    "randomquery_raw = \"Random Query\"\n",
    "random_query = encode(randomquery_raw)\n",
    "large_embed = tf.constant(np.random.random((100000,384)), dtype=tf.float32)\n",
    "start = time.time()\n",
    "scores = (random_query @ tf.transpose(large_embed))[0].numpy().tolist()\n",
    "end = time.time()\n",
    "print(f\"{end-start:.2f}s to compare a worst cast dot product with 100000 rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9251cb4-301c-4aa1-98b9-58be8a034820",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Making Show Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4b04f5ae-79db-48db-a5d7-de5c6653d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "show_descriptions = list(df_filtered['show_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "28228ae9-8944-4839-aaee-753c5dc60990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08s: 1 Show Embedding(s)\n",
      "0.09s: 3 Show Embedding(s)\n",
      "0.12s: 10 Show Embedding(s)\n",
      "0.27s: 30 Show Embedding(s)\n",
      "1.10s: 100 Show Embedding(s)\n",
      "9.62s: 300 Show Embedding(s)\n"
     ]
    }
   ],
   "source": [
    "# Note: on show description embedding size of 1000, my macbook ran out of memory.\n",
    "t = []\n",
    "for i in [1, 3, 10, 30, 100, 300]:\n",
    "    start = time.time()\n",
    "    shows_embeddings = encode(show_descriptions[:i])\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print(f\"{end-start:.2f}s: {i} Show Embedding(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "66fc0b4b-bbbd-484e-a521-d608a129a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Show Embeddings size: 30.  Maximizes Embeddings Per Second on my local machine\n"
     ]
    }
   ],
   "source": [
    "embedding_size_trials = np.array([1, 3, 10, 30, 100, 300])\n",
    "# take the embedding size trials and then take the argmax of the Embeddings Per Second Metric\n",
    "# which gives you the best index from embedding_size_trials.  this becomes the best_size.\n",
    "embeddings_per_sec = embedding_size_trials / np.array(t)\n",
    "best_index = np.argmax(embeddings_per_sec)\n",
    "best_size = embedding_size_trials[best_index]\n",
    "print(f\"Best Show Embeddings size: {best_size}.  Maximizes Embeddings Per Second on my local machine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e25ef85-00d2-4c0e-b06f-3bc8b19fbd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39 minutes to create all show embeddings.\n"
     ]
    }
   ],
   "source": [
    "# To attempt embeddings for each show...\n",
    "# create 30 embeddings at a time would take: \n",
    "print(f\"{df_filtered.shape[0] / embeddings_per_sec[best_index] / 60 :.2f} minutes to create all show embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401edc7-9a56-4fc9-ac60-81f4ab8f77b0",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Making Episode Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9c776b4f-ad81-4403-ae57-68b1d132406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "episode_descriptions = list(df['episode_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3e80961b-ba11-4b1c-a661-39c8b098ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16s: 1 Episode Embedding(s)\n",
      "0.12s: 3 Episode Embedding(s)\n",
      "0.20s: 10 Episode Embedding(s)\n",
      "0.76s: 30 Episode Embedding(s)\n",
      "4.25s: 100 Episode Embedding(s)\n"
     ]
    }
   ],
   "source": [
    "# Note: on show description embedding size of 1000, my macbook ran out of memory.\n",
    "t = []\n",
    "for i in [1, 3, 10, 30, 100]:\n",
    "    start = time.time()\n",
    "    episode_embeddings = encode(episode_descriptions[:i])\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print(f\"{end-start:.2f}s: {i} Episode Embedding(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "35e766e4-f609-4caf-b365-ee4175c9eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Episode Embeddings size: 10.  Maximizes Embeddings Per Second on my local machine\n"
     ]
    }
   ],
   "source": [
    "embedding_size_trials = np.array([1, 3, 10, 30, 100])\n",
    "# take the embedding size trials and then take the argmax of the Embeddings Per Second Metric\n",
    "# which gives you the best index from embedding_size_trials.  this becomes the best_size.\n",
    "embeddings_per_sec = embedding_size_trials / np.array(t)\n",
    "best_index = np.argmax(embeddings_per_sec)\n",
    "best_size = embedding_size_trials[best_index]\n",
    "print(f\"Best Episode Embeddings size: {best_size}.  Maximizes Embeddings Per Second on my local machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bc292021-b450-4dc6-ad21-ac63b9168335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.56 minutes to create all episode embeddings.\n"
     ]
    }
   ],
   "source": [
    "# To attempt embeddings for each show...\n",
    "# create 30 embeddings at a time would take: \n",
    "print(f\"{df.shape[0] / embeddings_per_sec[best_index] / 60 :.2f} minutes to create all episode embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68d1c6-0dc2-461f-8979-f4242a575a4d",
   "metadata": {},
   "source": [
    "# Test Code for Generating encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7bbd3bc5-4068-411f-8354-13b49ff1f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with text key (column: show_name) and text to embed (column: show_description) \n",
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "\n",
    "# Create blocks of data and call encode\n",
    "# Save the data in a systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e16c77ef-05f9-475c-9f49-e432cff9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block size\n",
    "block_size = 30\n",
    "\n",
    "# Iterate over consecutive blocks of rows\n",
    "num_rows = len(df_filtered)\n",
    "start_index = 0\n",
    "counter = 0\n",
    "while start_index < num_rows:\n",
    "    end_index = start_index + block_size if start_index + block_size < num_rows else num_rows\n",
    "    subset_df = df.iloc[start_index:end_index]\n",
    "\n",
    "    # Apply the encode function to the current block\n",
    "    #encode_function(subset_df)\n",
    "    #save_data(subset_df\n",
    "    start_index = end_index\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f7cbcaef-08de-4b04-924e-5c8b649b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data structure of embeddings being saved.\n",
    "embedding_size = 384\n",
    "block_size = 30\n",
    "num_blocks = 3\n",
    "num_padding = 5\n",
    "files = []\n",
    "for i in range(num_blocks):\n",
    "    # fake data\n",
    "    d = {\n",
    "        \"block\":i,\n",
    "        \"show_names\":['show1','show2','show3'] * 10,\n",
    "        \"show_desc_embeddings\": np.ones((block_size,embedding_size))*i\n",
    "    }\n",
    "    block_num = d['block']\n",
    "    filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "    files.append(filename)\n",
    "    # Save to disk\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6bca1076-b984-414a-889c-d80017afc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 384) 90\n"
     ]
    }
   ],
   "source": [
    "def extract_shows(filenames):\n",
    "    \"\"\"\n",
    "    input args:\n",
    "        files - list of files used to save the embeddings.\n",
    "    return args: \n",
    "        embedding_matrix - a numpy array of size (block_size x num_blocks,embedding_size) \n",
    "        list_of_shows - a list of strings of len (block_size x num_blocks)\n",
    "    \"\"\"\n",
    "    # This code takes in a list of files, then loads them, \n",
    "    # extracts the embeddings and concatenates them with the other embeddings.\n",
    "    list_of_tensors = []\n",
    "    list_of_shows = []\n",
    "    for file in filenames: \n",
    "        # Load from disk\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        list_of_tensors.append(loaded_data['show_desc_embeddings'])\n",
    "        list_of_shows.extend(loaded_data['show_names'])\n",
    "    \n",
    "    embedding_matrix = np.vstack(list_of_tensors)\n",
    "    return embedding_matrix, list_of_shows\n",
    "\n",
    "a,b= extract_shows(files)\n",
    "print(a.shape, len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d9621-054e-44a3-b2bf-3b518031a461",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f8f700ce-41bf-4879-a1fd-a64f4a603b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Embeddings in Chunks: 908it [2:53:44, 11.48s/it]                                           \n",
      "Combining Embeddings: 100%|████████████████████████████████████| 908/908 [00:01<00:00, 750.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10425.814870119095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EmbeddingGen:\n",
    "    def __init__(self, df, data_key, label_key, block_size=1, encoder=None):\n",
    "        self.data_frame = df\n",
    "        self.data_key = data_key\n",
    "        self.label_key = label_key\n",
    "        self.block_size = block_size\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.files = []\n",
    "        self.embedding_matrix = []\n",
    "        self.embedding_labels = []\n",
    "        # \n",
    "        self._saveEmbeddingChunks()\n",
    "        self._combineEmbeddings()\n",
    "\n",
    "    def load(self,file):\n",
    "        \"\"\"\n",
    "        Loading embeddings from file.\n",
    "        \"\"\"\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        self.embedding_matrix = loaded_data['embeddings']\n",
    "        self.embedding_labels = loaded_data['embedding_labels']\n",
    "    \n",
    "    def compare(self,query,n=5):\n",
    "        df = self.data_frame\n",
    "        dataemb = tf.constant(self.embedding_matrix,dtype=\"float32\")\n",
    "        #Compute dot score between query and all document embeddings\n",
    "        scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        lst = []\n",
    "        for i in sorted_indices[:n]:\n",
    "            #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "            #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "            #print('')\n",
    "            #print('')\n",
    "            lst.append({\n",
    "                \"label\": self.embedding_labels[i],\n",
    "                \"score\": scores[i],\n",
    "                \"data\":df[df[self.label_key]==self.embedding_labels[i]][self.data_key].iloc[0]\n",
    "            })\n",
    "        return lst\n",
    "    \n",
    "    def _saveEmbeddingChunks(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate over consecutive blocks of rows\n",
    "        num_rows = len(self.data_frame)\n",
    "        start_index = 0\n",
    "        block_counter = 0\n",
    "        est_total_iterations = num_rows // self.block_size\n",
    "        # Initialize tqdm with the total number of iterations\n",
    "        with tqdm.tqdm(total=est_total_iterations, desc=\"Saving Embeddings in Chunks\") as pbar:\n",
    "        # Start your while loop\n",
    "            while start_index < num_rows:\n",
    "                end_index = start_index + self.block_size if start_index + self.block_size < num_rows else num_rows\n",
    "                subset_df = self.data_frame.iloc[start_index:end_index]\n",
    "            \n",
    "                # Apply the encode function to the current block\n",
    "                emb = self.encoder(list(subset_df[self.data_key]))\n",
    "                emb_labels = list(subset_df[self.label_key])\n",
    "                self._saveChunk(emb,emb_labels,block_counter)\n",
    "                \n",
    "                start_index = end_index\n",
    "                block_counter += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    def save(self):\n",
    "        self._saveChunk(self.embedding_matrix, self.embedding_labels, filename=\"final.pkl\")\n",
    "    \n",
    "    def _saveChunk(self, embeddings, embedding_labels, block_num=0, filename=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"block\":block_num,\n",
    "            \"embedding_labels\":embedding_labels,\n",
    "            \"embeddings\": embeddings\n",
    "        }\n",
    "        block_num = d['block']\n",
    "        if not filename: \n",
    "            filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "            self.files.append(filename)\n",
    "            \n",
    "        # Save to disk\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(d, f)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        # Explicit method for cleaning up resources\n",
    "        for file in self.files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                #print(f\"File {file} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file}: {e}\")\n",
    "                \n",
    "    def _combineEmbeddings(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # This code takes in a list of files, then loads them, \n",
    "        # extracts the embeddings and concatenates them with the other embeddings.\n",
    "        list_of_embeddings = []\n",
    "        list_of_labels = []\n",
    "        for file in tqdm.tqdm(self.files,desc=\"Combining Embeddings\"): \n",
    "            # Load from disk\n",
    "            with open(file, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "            list_of_embeddings.append(loaded_data['embeddings'])\n",
    "            list_of_labels.extend(loaded_data['embedding_labels'])\n",
    "        \n",
    "        self.embedding_matrix = np.vstack(list_of_embeddings)\n",
    "        self.embedding_labels = list_of_labels\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "embgen = EmbeddingGen(df, data_key='episode_description', label_key='episode_name', block_size=100, encoder=encode)\n",
    "embgen.save()\n",
    "end = time.time()\n",
    "print(f\"{end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "526fe767-3ac2-4978-9b79-2aabe39656b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embgen.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "73d5a3fe-0f59-43a0-b056-1830f5f5f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This week Murder Friends covers two of our favourites: Dectective '\n",
      "          \"Trapp and Don't F*ck With Cats. Also: the birds have hats. It's \"\n",
      "          'important. ',\n",
      "  'label': 'Ep 12 - The Birds Have Hats, Bro!',\n",
      "  'score': 0.507026731967926},\n",
      " {'data': 'We sit in a bathtub and pet our cat.\\xa0 ',\n",
      "  'label': 'Bathtime: freshman year',\n",
      "  'score': 0.4883423149585724},\n",
      " {'data': 'The sick cat ', 'label': 'Kiki ', 'score': 0.4700942039489746},\n",
      " {'data': 'This episode I have a special guest, Insana Collins (@insanaC) & we '\n",
      "          'discuss the importance of empowering your cat to be purrrrfect!  ',\n",
      "  'label': 'Snacks, Cats & Insana',\n",
      "  'score': 0.4659312069416046},\n",
      " {'data': \"Khora. She loves cats. And there's a damned good reason why you \"\n",
      "          \"should love cats too. (Spoiler: it's because Khora's cat is \"\n",
      "          'amazing.) Listen to find out why.\\xa0 ',\n",
      "  'label': 'Mini 24 - Khora',\n",
      "  'score': 0.45654088258743286}]\n"
     ]
    }
   ],
   "source": [
    "query_episode = 'cats are the best.  this is how to take care of cats in a hurricane.'\n",
    "query = encode(query_episode)\n",
    "pprint.pprint(embgen.compare(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "7a005470-d6af-422c-8d52-4b952618b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Frillz Podcast with Yipes & Matrix',\n",
       " 'Ayodya Talk',\n",
       " 'Coca-cola',\n",
       " 'The Motivational Dude Podcast',\n",
       " 'Uwu',\n",
       " 'The Phoenix Project Podcast',\n",
       " 'Is it in yet? A sex podcast',\n",
       " 'Star Wars Sessions',\n",
       " 'The Culture Project Podcast',\n",
       " 'Women That Wait (WTW)']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embgen.embedding_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2f923dbb-ea41-4756-a435-0a97d18b299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannabis Investing Network: 2 Private Cannabis Investors share thoughts, analysis and opinions on the ups and downs of the rapidly changing Cannabis Investing landscape. For Investors By Investors\n"
     ]
    }
   ],
   "source": [
    "REF_INDEX = 505\n",
    "desc = df_filtered['show_description'].iloc[REF_INDEX]\n",
    "name = df_filtered['show_name'].iloc[REF_INDEX]\n",
    "print(f\"{name}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0aae9c0d-0ff4-4605-929a-3a1b9bc274bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query0_raw = \"Private Cannabis Investors share thoughts.\"\n",
    "query0 = encode(query0_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3848b494-826c-4b6c-a642-19662e9fa9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,   7, 154, 143,  32,  80,  44, 142,  13, 119, 130,  10,  85,\n",
       "       102, 109,  54, 104, 147, 123,  68,  82, 126,  84,  41,  93,  98,\n",
       "       118,  15, 146,  90,  55,   4, 134,  30, 149, 108, 141,  42,  67,\n",
       "       111, 144,  23,  25, 128,   3, 124,  22,  53,  18, 138, 150,  59,\n",
       "        56,  65,  12,  75,   9,  36, 115,  89,  95, 114,   8,  52,  29,\n",
       "       151, 113,  81, 135, 127,  49, 133,  39,   1,  63, 101,  47, 148,\n",
       "        14, 139,  57, 152,  37,  46,  19,  35,  77,  27, 122,  96,  69,\n",
       "       140,  83, 137, 153,  20, 103,  72,   5, 131,  34,  51, 110, 116,\n",
       "        48,  99,  94,  97,  38,  40,   6, 125,  60,  66,  88, 106, 121,\n",
       "        16, 129, 107,  33,  91,  71,  61,  45,  31,  24,  79,  87,  76,\n",
       "        62,  73,  43,  74,  58, 112,  64,  70,  21, 132, 120,   0,  26,\n",
       "        17, 117, 136,  86, 145,  92,  11,   2, 105,  50,  78,  28])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataemb = tf.constant(embgen.embedding_matrix,dtype=\"float32\")\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query0 @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "#display(scores)\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "06887eba-8a2a-4c12-a414-7863f6b0b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights with Joe Pane: 0.79\n",
      "This podcast is dedicated to those of us on a journey from ambition to meaning. I share with you the experiences of 1000's of people I have had the honor of coaching, training and leading over the last decade and a half, who have each embarked on this journey. This podcast is about redefining success. Ultimately, success is about the value we have been to someone else. This kind of success flavors our ultimate legacy, which is the contribution we have made to the live's of others. Thank you and I look forward to sharing all I can about this beautiful journey. \n",
      "\n",
      "\n",
      "The Good Sign : 0.66\n",
      "Let’s be real and honest! Life can be challenging and stressful! I am a mom, a teacher, a life coach and a motivational speaker. Meeting so many people from so many places has made me realize that we are all on a similar quest for happiness and inspiration. This podcast will be uplifting, honest and damn funny! Join me each Monday night as me and my guests impart attainable goals and strategies that will uplift you and make you smile. \n",
      "\n",
      "\n",
      "United in Motherhood by Zoe Young : 0.58\n",
      "The podcast for every woman, for every Mama. The #unitedinmotherhood podcast is a safe space. It's where I share with you, the stories of incredible women. Women that have us all seeing the world in a different way. Sharing their stories to empower, drive connection and uplift us all to feel United In Motherhood. Cause let's face it, together we are stronger! \n",
      "\n",
      "\n",
      "The Lyndsey Morrison Podcast: 0.57\n",
      "This Podcast is for Group Fitness Instructors. I Share Insight and Experiences From My Own Group Fitness Journey and Interview Fitness Leaders That Are Impacting The Fitness Arena.  \n",
      "\n",
      "\n",
      "Always Be Raising Podcast - by G&H Ventures: 0.56\n",
      "Always Be Raising Podcast is a series by G&H Ventures, an early-stage tech investor in Vietnam. We will interview top venture capitalists and founders in Vietnam on startup fundraising, operations, tech trends, and the future.   Join us and share your story!   Contact me at: louis.nguyen@ghventures.vc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_indices[:5]:\n",
    "    print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "    print(df_filtered[df_filtered['show_name']==embgen.embedding_labels[i]].show_description.iloc[0])\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b46b3e15-5002-451e-94e9-5171a337535f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This podcast is dedicated to those of us on a journey from ambition '\n",
      "          \"to meaning. I share with you the experiences of 1000's of people I \"\n",
      "          'have had the honor of coaching, training and leading over the last '\n",
      "          'decade and a half, who have each embarked on this journey. This '\n",
      "          'podcast is about redefining success. Ultimately, success is about '\n",
      "          'the value we have been to someone else. This kind of success '\n",
      "          'flavors our ultimate legacy, which is the contribution we have made '\n",
      "          \"to the live's of others. Thank you and I look forward to sharing \"\n",
      "          'all I can about this beautiful journey. ',\n",
      "  'label': 'Insights with Joe Pane',\n",
      "  'score': 0.7919762134552002},\n",
      " {'data': 'Let’s be real and honest! Life can be challenging and stressful! I '\n",
      "          'am a mom, a teacher, a life coach and a motivational speaker. '\n",
      "          'Meeting so many people from so many places has made me realize that '\n",
      "          'we are all on a similar quest for happiness and inspiration. This '\n",
      "          'podcast will be uplifting, honest and damn funny! Join me each '\n",
      "          'Monday night as me and my guests impart attainable goals and '\n",
      "          'strategies that will uplift you and make you smile. ',\n",
      "  'label': 'The Good Sign ',\n",
      "  'score': 0.6577978134155273},\n",
      " {'data': 'The podcast for every woman, for every Mama. The '\n",
      "          \"#unitedinmotherhood podcast is a safe space. It's where I share \"\n",
      "          'with you, the stories of incredible women. Women that have us all '\n",
      "          'seeing the world in a different way. Sharing their stories to '\n",
      "          'empower, drive connection and uplift us all to feel United In '\n",
      "          \"Motherhood. Cause let's face it, together we are stronger! \",\n",
      "  'label': 'United in Motherhood by Zoe Young ',\n",
      "  'score': 0.5826876759529114},\n",
      " {'data': 'This Podcast is for Group Fitness Instructors. I Share Insight and '\n",
      "          'Experiences From My Own Group Fitness Journey and Interview Fitness '\n",
      "          'Leaders That Are Impacting The Fitness Arena.  ',\n",
      "  'label': 'The Lyndsey Morrison Podcast',\n",
      "  'score': 0.5743718147277832},\n",
      " {'data': 'Always Be Raising Podcast is a series by G&H Ventures, an '\n",
      "          'early-stage tech investor in Vietnam. We will interview top venture '\n",
      "          'capitalists and founders in Vietnam on startup fundraising, '\n",
      "          'operations, tech trends, and the future.   Join us and share your '\n",
      "          'story!   Contact me at: louis.nguyen@ghventures.vc',\n",
      "  'label': 'Always Be Raising Podcast - by G&H Ventures',\n",
      "  'score': 0.5638134479522705}]\n"
     ]
    }
   ],
   "source": [
    "def find_top_n(objs,query,n=5):\n",
    "    df = objs.data_frame\n",
    "    dataemb = tf.constant(objs.embedding_matrix,dtype=\"float32\")\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    lst = []\n",
    "    for i in sorted_indices[:n]:\n",
    "        #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "        #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "        #print('')\n",
    "        #print('')\n",
    "        lst.append({\n",
    "            \"label\": embgen.embedding_labels[i],\n",
    "            \"score\": scores[i],\n",
    "            \"data\":df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0]\n",
    "        })\n",
    "    return lst\n",
    "import pprint\n",
    "pprint.pprint(find_top_n(embgen, query0, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6d77be43-0bc5-43a1-a575-ecf9a65d7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This podcast is dedicated to those of us on a journey from ambition '\n",
      "          \"to meaning. I share with you the experiences of 1000's of people I \"\n",
      "          'have had the honor of coaching, training and leading over the last '\n",
      "          'decade and a half, who have each embarked on this journey. This '\n",
      "          'podcast is about redefining success. Ultimately, success is about '\n",
      "          'the value we have been to someone else. This kind of success '\n",
      "          'flavors our ultimate legacy, which is the contribution we have made '\n",
      "          \"to the live's of others. Thank you and I look forward to sharing \"\n",
      "          'all I can about this beautiful journey. ',\n",
      "  'label': 'Insights with Joe Pane',\n",
      "  'score': 0.7919762134552002},\n",
      " {'data': 'Let’s be real and honest! Life can be challenging and stressful! I '\n",
      "          'am a mom, a teacher, a life coach and a motivational speaker. '\n",
      "          'Meeting so many people from so many places has made me realize that '\n",
      "          'we are all on a similar quest for happiness and inspiration. This '\n",
      "          'podcast will be uplifting, honest and damn funny! Join me each '\n",
      "          'Monday night as me and my guests impart attainable goals and '\n",
      "          'strategies that will uplift you and make you smile. ',\n",
      "  'label': 'The Good Sign ',\n",
      "  'score': 0.6577978134155273},\n",
      " {'data': 'The podcast for every woman, for every Mama. The '\n",
      "          \"#unitedinmotherhood podcast is a safe space. It's where I share \"\n",
      "          'with you, the stories of incredible women. Women that have us all '\n",
      "          'seeing the world in a different way. Sharing their stories to '\n",
      "          'empower, drive connection and uplift us all to feel United In '\n",
      "          \"Motherhood. Cause let's face it, together we are stronger! \",\n",
      "  'label': 'United in Motherhood by Zoe Young ',\n",
      "  'score': 0.5826876759529114},\n",
      " {'data': 'This Podcast is for Group Fitness Instructors. I Share Insight and '\n",
      "          'Experiences From My Own Group Fitness Journey and Interview Fitness '\n",
      "          'Leaders That Are Impacting The Fitness Arena.  ',\n",
      "  'label': 'The Lyndsey Morrison Podcast',\n",
      "  'score': 0.5743718147277832},\n",
      " {'data': 'Always Be Raising Podcast is a series by G&H Ventures, an '\n",
      "          'early-stage tech investor in Vietnam. We will interview top venture '\n",
      "          'capitalists and founders in Vietnam on startup fundraising, '\n",
      "          'operations, tech trends, and the future.   Join us and share your '\n",
      "          'story!   Contact me at: louis.nguyen@ghventures.vc',\n",
      "  'label': 'Always Be Raising Podcast - by G&H Ventures',\n",
      "  'score': 0.5638134479522705}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(embgen.compare(query0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
