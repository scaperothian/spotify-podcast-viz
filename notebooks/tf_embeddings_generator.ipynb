{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22ea4f7-3b6b-4554-aeda-ae85e7f96f0a",
   "metadata": {},
   "source": [
    "# Work on embeddings\n",
    "\n",
    "Objective: Take Podcast Descriptions and create embeddings from them.  Create an application that allows a user to enter their own text and use cosine similarity to find the most similar show descriptions (or episodes based on episode description).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf087d1-9581-4d00-89e4-11765da5cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07f9b4-640b-46ac-b46b-4cc10fb3cfc0",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea783ca8-cf7b-44f8-9908-f1c9eb5da55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90706, 18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../metadata_with_episode_dates_and_category.tsv',sep='\\t')\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], format='%Y-%m-%d').reset_index(drop=True)\n",
    "df = df[~df['release_date'].isna()]\n",
    "df = df[~df['category'].isna()]\n",
    "df = df[~df['show_description'].isna()]\n",
    "df = df[~df['show_name'].isna()]\n",
    "df = df[~df['episode_description'].isna()]\n",
    "df = df[~df['episode_name'].isna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263340d-14d6-4bf3-bcb7-249b15fed6e0",
   "metadata": {},
   "source": [
    "# Define Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc47852-b743-48d4-8e03-eecc2c71dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 22:29:48.912464: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-11-23 22:29:48.912484: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-11-23 22:29:48.912489: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-11-23 22:29:48.912643: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-23 22:29:48.912785: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = TFAutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9c776b4f-ad81-4403-ae57-68b1d132406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "episode_descriptions = list(df['episode_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68d1c6-0dc2-461f-8979-f4242a575a4d",
   "metadata": {},
   "source": [
    "# Test Code for Generating encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbd3bc5-4068-411f-8354-13b49ff1f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with text key (column: show_name) and text to embed (column: show_description) \n",
    "df_shows = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "\n",
    "# Create blocks of data and call encode\n",
    "# Save the data in a systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e16c77ef-05f9-475c-9f49-e432cff9abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 iterations.\n",
      "....."
     ]
    }
   ],
   "source": [
    "df_test = df_shows.iloc[:146]\n",
    "# Block size (see benchmarking)\n",
    "block_size = 30\n",
    "# Iterate over consecutive blocks of rows\n",
    "num_rows = len(df_test)\n",
    "print(f\"Expect {num_rows // block_size+1} iterations.\")\n",
    "\n",
    "start_index = 0\n",
    "counter = 0\n",
    "while start_index < num_rows:\n",
    "    end_index = start_index + block_size if start_index + block_size < num_rows else num_rows\n",
    "    subset_df = df_test.iloc[start_index:end_index]\n",
    "\n",
    "    # Apply the encode function to the current block\n",
    "    encode(list(subset_df['show_description']))\n",
    "    print('.',end='')\n",
    "    #save_data(subset_df\n",
    "    start_index = end_index\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f7cbcaef-08de-4b04-924e-5c8b649b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data structure of embeddings being saved.\n",
    "embedding_size = 384\n",
    "block_size = 30\n",
    "num_blocks = 3\n",
    "num_padding = 5\n",
    "files = []\n",
    "for i in range(num_blocks):\n",
    "    # fake data\n",
    "    d = {\n",
    "        \"block\":i,\n",
    "        \"show_names\":['show1','show2','show3'] * 10,\n",
    "        \"show_desc_embeddings\": np.ones((block_size,embedding_size))*i\n",
    "    }\n",
    "    block_num = d['block']\n",
    "    filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "    files.append(filename)\n",
    "    # Save to disk\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6bca1076-b984-414a-889c-d80017afc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 384) 90\n"
     ]
    }
   ],
   "source": [
    "def extract_shows(filenames):\n",
    "    \"\"\"\n",
    "    input args:\n",
    "        files - list of files used to save the embeddings.\n",
    "    return args: \n",
    "        embedding_matrix - a numpy array of size (block_size x num_blocks,embedding_size) \n",
    "        list_of_shows - a list of strings of len (block_size x num_blocks)\n",
    "    \"\"\"\n",
    "    # This code takes in a list of files, then loads them, \n",
    "    # extracts the embeddings and concatenates them with the other embeddings.\n",
    "    list_of_tensors = []\n",
    "    list_of_shows = []\n",
    "    for file in filenames: \n",
    "        # Load from disk\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        list_of_tensors.append(loaded_data['show_desc_embeddings'])\n",
    "        list_of_shows.extend(loaded_data['show_names'])\n",
    "    \n",
    "    embedding_matrix = np.vstack(list_of_tensors)\n",
    "    return embedding_matrix, list_of_shows\n",
    "\n",
    "a,b= extract_shows(files)\n",
    "print(a.shape, len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d9621-054e-44a3-b2bf-3b518031a461",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f8f700ce-41bf-4879-a1fd-a64f4a603b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Embeddings in Chunks: 908it [2:53:44, 11.48s/it]                                           \n",
      "Combining Embeddings: 100%|████████████████████████████████████| 908/908 [00:01<00:00, 750.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10425.814870119095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EmbeddingGen:\n",
    "    def __init__(self, df, data_key, label_key, block_size=1, encoder=None):\n",
    "        self.data_frame = df\n",
    "        self.data_key = data_key\n",
    "        self.label_key = label_key\n",
    "        self.block_size = block_size\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.files = []\n",
    "        self.embedding_matrix = []\n",
    "        self.embedding_labels = []\n",
    "        # \n",
    "        self._saveEmbeddingChunks()\n",
    "        self._combineEmbeddings()\n",
    "\n",
    "    def load(self,file):\n",
    "        \"\"\"\n",
    "        Loading embeddings from file.\n",
    "        \"\"\"\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        self.embedding_matrix = loaded_data['embeddings']\n",
    "        self.embedding_labels = loaded_data['embedding_labels']\n",
    "    \n",
    "    def compare(self,query,n=5):\n",
    "        df = self.data_frame\n",
    "        dataemb = tf.constant(self.embedding_matrix,dtype=\"float32\")\n",
    "        #Compute dot score between query and all document embeddings\n",
    "        scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        lst = []\n",
    "        for i in sorted_indices[:n]:\n",
    "            #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "            #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "            #print('')\n",
    "            #print('')\n",
    "            lst.append({\n",
    "                \"label\": self.embedding_labels[i],\n",
    "                \"score\": scores[i],\n",
    "                \"data\":df[df[self.label_key]==self.embedding_labels[i]][self.data_key].iloc[0]\n",
    "            })\n",
    "        return lst\n",
    "    \n",
    "    def _saveEmbeddingChunks(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate over consecutive blocks of rows\n",
    "        num_rows = len(self.data_frame)\n",
    "        start_index = 0\n",
    "        block_counter = 0\n",
    "        est_total_iterations = num_rows // self.block_size\n",
    "        # Initialize tqdm with the total number of iterations\n",
    "        with tqdm.tqdm(total=est_total_iterations, desc=\"Saving Embeddings in Chunks\") as pbar:\n",
    "        # Start your while loop\n",
    "            while start_index < num_rows:\n",
    "                end_index = start_index + self.block_size if start_index + self.block_size < num_rows else num_rows\n",
    "                subset_df = self.data_frame.iloc[start_index:end_index]\n",
    "            \n",
    "                # Apply the encode function to the current block\n",
    "                emb = self.encoder(list(subset_df[self.data_key]))\n",
    "                emb_labels = list(subset_df[self.label_key])\n",
    "                self._saveChunk(emb,emb_labels,block_counter)\n",
    "                \n",
    "                start_index = end_index\n",
    "                block_counter += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    def save(self):\n",
    "        self._saveChunk(self.embedding_matrix, self.embedding_labels, filename=\"final.pkl\")\n",
    "    \n",
    "    def _saveChunk(self, embeddings, embedding_labels, block_num=0, filename=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"block\":block_num,\n",
    "            \"embedding_labels\":embedding_labels,\n",
    "            \"embeddings\": embeddings\n",
    "        }\n",
    "        block_num = d['block']\n",
    "        if not filename: \n",
    "            filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "            self.files.append(filename)\n",
    "            \n",
    "        # Save to disk\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(d, f)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        # Explicit method for cleaning up resources\n",
    "        for file in self.files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                #print(f\"File {file} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file}: {e}\")\n",
    "                \n",
    "    def _combineEmbeddings(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # This code takes in a list of files, then loads them, \n",
    "        # extracts the embeddings and concatenates them with the other embeddings.\n",
    "        list_of_embeddings = []\n",
    "        list_of_labels = []\n",
    "        for file in tqdm.tqdm(self.files,desc=\"Combining Embeddings\"): \n",
    "            # Load from disk\n",
    "            with open(file, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "            list_of_embeddings.append(loaded_data['embeddings'])\n",
    "            list_of_labels.extend(loaded_data['embedding_labels'])\n",
    "        \n",
    "        self.embedding_matrix = np.vstack(list_of_embeddings)\n",
    "        self.embedding_labels = list_of_labels\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "embgen = EmbeddingGen(df, data_key='episode_description', label_key='episode_name', block_size=100, encoder=encode)\n",
    "embgen.save()\n",
    "end = time.time()\n",
    "print(f\"{end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "526fe767-3ac2-4978-9b79-2aabe39656b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embgen.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73d5a3fe-0f59-43a0-b056-1830f5f5f926",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embgen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcats are the best.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m encode(query_episode)\n\u001b[0;32m----> 3\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[43membgen\u001b[49m\u001b[38;5;241m.\u001b[39mcompare(query))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embgen' is not defined"
     ]
    }
   ],
   "source": [
    "query_episode = 'cats are the best.'\n",
    "query = encode(query_episode)\n",
    "pprint.pprint(embgen.compare(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "7a005470-d6af-422c-8d52-4b952618b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Frillz Podcast with Yipes & Matrix',\n",
       " 'Ayodya Talk',\n",
       " 'Coca-cola',\n",
       " 'The Motivational Dude Podcast',\n",
       " 'Uwu',\n",
       " 'The Phoenix Project Podcast',\n",
       " 'Is it in yet? A sex podcast',\n",
       " 'Star Wars Sessions',\n",
       " 'The Culture Project Podcast',\n",
       " 'Women That Wait (WTW)']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embgen.embedding_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2f923dbb-ea41-4756-a435-0a97d18b299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannabis Investing Network: 2 Private Cannabis Investors share thoughts, analysis and opinions on the ups and downs of the rapidly changing Cannabis Investing landscape. For Investors By Investors\n"
     ]
    }
   ],
   "source": [
    "REF_INDEX = 505\n",
    "desc = df_filtered['show_description'].iloc[REF_INDEX]\n",
    "name = df_filtered['show_name'].iloc[REF_INDEX]\n",
    "print(f\"{name}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0aae9c0d-0ff4-4605-929a-3a1b9bc274bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query0_raw = \"Private Cannabis Investors share thoughts.\"\n",
    "query0 = encode(query0_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3848b494-826c-4b6c-a642-19662e9fa9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,   7, 154, 143,  32,  80,  44, 142,  13, 119, 130,  10,  85,\n",
       "       102, 109,  54, 104, 147, 123,  68,  82, 126,  84,  41,  93,  98,\n",
       "       118,  15, 146,  90,  55,   4, 134,  30, 149, 108, 141,  42,  67,\n",
       "       111, 144,  23,  25, 128,   3, 124,  22,  53,  18, 138, 150,  59,\n",
       "        56,  65,  12,  75,   9,  36, 115,  89,  95, 114,   8,  52,  29,\n",
       "       151, 113,  81, 135, 127,  49, 133,  39,   1,  63, 101,  47, 148,\n",
       "        14, 139,  57, 152,  37,  46,  19,  35,  77,  27, 122,  96,  69,\n",
       "       140,  83, 137, 153,  20, 103,  72,   5, 131,  34,  51, 110, 116,\n",
       "        48,  99,  94,  97,  38,  40,   6, 125,  60,  66,  88, 106, 121,\n",
       "        16, 129, 107,  33,  91,  71,  61,  45,  31,  24,  79,  87,  76,\n",
       "        62,  73,  43,  74,  58, 112,  64,  70,  21, 132, 120,   0,  26,\n",
       "        17, 117, 136,  86, 145,  92,  11,   2, 105,  50,  78,  28])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataemb = tf.constant(embgen.embedding_matrix,dtype=\"float32\")\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query0 @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "#display(scores)\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "06887eba-8a2a-4c12-a414-7863f6b0b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights with Joe Pane: 0.79\n",
      "This podcast is dedicated to those of us on a journey from ambition to meaning. I share with you the experiences of 1000's of people I have had the honor of coaching, training and leading over the last decade and a half, who have each embarked on this journey. This podcast is about redefining success. Ultimately, success is about the value we have been to someone else. This kind of success flavors our ultimate legacy, which is the contribution we have made to the live's of others. Thank you and I look forward to sharing all I can about this beautiful journey. \n",
      "\n",
      "\n",
      "The Good Sign : 0.66\n",
      "Let’s be real and honest! Life can be challenging and stressful! I am a mom, a teacher, a life coach and a motivational speaker. Meeting so many people from so many places has made me realize that we are all on a similar quest for happiness and inspiration. This podcast will be uplifting, honest and damn funny! Join me each Monday night as me and my guests impart attainable goals and strategies that will uplift you and make you smile. \n",
      "\n",
      "\n",
      "United in Motherhood by Zoe Young : 0.58\n",
      "The podcast for every woman, for every Mama. The #unitedinmotherhood podcast is a safe space. It's where I share with you, the stories of incredible women. Women that have us all seeing the world in a different way. Sharing their stories to empower, drive connection and uplift us all to feel United In Motherhood. Cause let's face it, together we are stronger! \n",
      "\n",
      "\n",
      "The Lyndsey Morrison Podcast: 0.57\n",
      "This Podcast is for Group Fitness Instructors. I Share Insight and Experiences From My Own Group Fitness Journey and Interview Fitness Leaders That Are Impacting The Fitness Arena.  \n",
      "\n",
      "\n",
      "Always Be Raising Podcast - by G&H Ventures: 0.56\n",
      "Always Be Raising Podcast is a series by G&H Ventures, an early-stage tech investor in Vietnam. We will interview top venture capitalists and founders in Vietnam on startup fundraising, operations, tech trends, and the future.   Join us and share your story!   Contact me at: louis.nguyen@ghventures.vc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_indices[:5]:\n",
    "    print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "    print(df_filtered[df_filtered['show_name']==embgen.embedding_labels[i]].show_description.iloc[0])\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46b3e15-5002-451e-94e9-5171a337535f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_top_n(objs,query,n=5):\n",
    "    df = objs.data_frame\n",
    "    dataemb = tf.constant(objs.embedding_matrix,dtype=\"float32\")\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    lst = []\n",
    "    for i in sorted_indices[:n]:\n",
    "        #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "        #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "        #print('')\n",
    "        #print('')\n",
    "        lst.append({\n",
    "            \"label\": embgen.embedding_labels[i],\n",
    "            \"score\": scores[i],\n",
    "            \"data\":df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0]\n",
    "        })\n",
    "    return lst\n",
    "#import pprint\n",
    "#pprint.pprint(find_top_n(embgen, query0, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6d77be43-0bc5-43a1-a575-ecf9a65d7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This podcast is dedicated to those of us on a journey from ambition '\n",
      "          \"to meaning. I share with you the experiences of 1000's of people I \"\n",
      "          'have had the honor of coaching, training and leading over the last '\n",
      "          'decade and a half, who have each embarked on this journey. This '\n",
      "          'podcast is about redefining success. Ultimately, success is about '\n",
      "          'the value we have been to someone else. This kind of success '\n",
      "          'flavors our ultimate legacy, which is the contribution we have made '\n",
      "          \"to the live's of others. Thank you and I look forward to sharing \"\n",
      "          'all I can about this beautiful journey. ',\n",
      "  'label': 'Insights with Joe Pane',\n",
      "  'score': 0.7919762134552002},\n",
      " {'data': 'Let’s be real and honest! Life can be challenging and stressful! I '\n",
      "          'am a mom, a teacher, a life coach and a motivational speaker. '\n",
      "          'Meeting so many people from so many places has made me realize that '\n",
      "          'we are all on a similar quest for happiness and inspiration. This '\n",
      "          'podcast will be uplifting, honest and damn funny! Join me each '\n",
      "          'Monday night as me and my guests impart attainable goals and '\n",
      "          'strategies that will uplift you and make you smile. ',\n",
      "  'label': 'The Good Sign ',\n",
      "  'score': 0.6577978134155273},\n",
      " {'data': 'The podcast for every woman, for every Mama. The '\n",
      "          \"#unitedinmotherhood podcast is a safe space. It's where I share \"\n",
      "          'with you, the stories of incredible women. Women that have us all '\n",
      "          'seeing the world in a different way. Sharing their stories to '\n",
      "          'empower, drive connection and uplift us all to feel United In '\n",
      "          \"Motherhood. Cause let's face it, together we are stronger! \",\n",
      "  'label': 'United in Motherhood by Zoe Young ',\n",
      "  'score': 0.5826876759529114},\n",
      " {'data': 'This Podcast is for Group Fitness Instructors. I Share Insight and '\n",
      "          'Experiences From My Own Group Fitness Journey and Interview Fitness '\n",
      "          'Leaders That Are Impacting The Fitness Arena.  ',\n",
      "  'label': 'The Lyndsey Morrison Podcast',\n",
      "  'score': 0.5743718147277832},\n",
      " {'data': 'Always Be Raising Podcast is a series by G&H Ventures, an '\n",
      "          'early-stage tech investor in Vietnam. We will interview top venture '\n",
      "          'capitalists and founders in Vietnam on startup fundraising, '\n",
      "          'operations, tech trends, and the future.   Join us and share your '\n",
      "          'story!   Contact me at: louis.nguyen@ghventures.vc',\n",
      "  'label': 'Always Be Raising Podcast - by G&H Ventures',\n",
      "  'score': 0.5638134479522705}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(embgen.compare(query0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57795638-f9ea-4ddf-8a55-12a01e2f2239",
   "metadata": {},
   "source": [
    "## Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac812da-a7ba-4e1e-9d9a-7c471bb4522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"podcast_episode_description_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa13f7c-7bd9-4702-b80b-0f3b3ad8ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "with open(filename, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03704c28-53a8-4f6e-8bac-0da771ff52e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90706, 384)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = loaded_data['embeddings']\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2827e86-8446-472e-82b5-8a8258a0d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'JT Foxx discusses popular topics in the New and about business. '\n",
      "          'Every topic on the news is usually only focused on one side of the '\n",
      "          'story, what we do is discuss both sides of the story. There’s isn’t '\n",
      "          'a right or wrong there is only the truth.  In the episode live from '\n",
      "          'NYC we discuss everything from Facebook launching a dating app, DJ '\n",
      "          'Flume at Burning Man, Andrew Yang, uBiome, and other hot topics '\n",
      "          'from politics to business.   ',\n",
      "  'label': 'Live from New York City discussing, Crypto, Lyft, Under Armor, '\n",
      "           'Uniqlo, Netflix and many more! ',\n",
      "  'score': 0.5742020010948181},\n",
      " {'data': 'News - Chat with the Rushtons - results - Stone MM - fixtures - '\n",
      "          'Facebook question - competition ',\n",
      "  'label': 'Episode 25 - Pod in the Bod inc Stone MM, The Rushtons & go team',\n",
      "  'score': 0.5113983154296875},\n",
      " {'data': 'Just a topic about Death Row... If you have any topics you want us '\n",
      "          'to talk about let us know and maybe your topic will get announced. '\n",
      "          'Be safe Yall and stay strong .  anchor.fm  ---   This episode is '\n",
      "          'sponsored by  · Anchor: The easiest way to make a podcast.  '\n",
      "          'https://anchor.fm/app  ---   Send in a voice message: '\n",
      "          'https://anchor.fm/nimbl/message Support this podcast: '\n",
      "          'https://anchor.fm/nimbl/support',\n",
      "  'label': 'episode #0 (intro)',\n",
      "  'score': 0.5076537132263184},\n",
      " {'data': 'Podcast listeners, this is a in depth conversation where we go '\n",
      "          'in-depth on some of my favorite topics! How to win on LinkedIn, '\n",
      "          'Text, and TikTok, delivering value upfront, and more!! Tweet me '\n",
      "          \"@garyvee with you favorite moment.  Topics from Today's Episode:  \"\n",
      "          '00:30 - 5:30 | Speaker Introduces Gary  7:15 | Delivering Value '\n",
      "          'Upfront  7:55 | 3.3 milion to 9.7 at Wine Library  9:45 | How I '\n",
      "          'think about employees  12:25 | Building media empire  13:30 | '\n",
      "          'Content marketing on LinkedIn  16:45 | Comic strips crush on '\n",
      "          \"LinkedIn  18:40 | Scaling a business  22:00 | Don't convince  24:00 \"\n",
      "          '| Building safety within organizations  27:45 | Finding the right '\n",
      "          'talent  30:00 | How I hire & fire  37:00 | Consumer attention is my '\n",
      "          'religion  38:45 | People focused culture  41:00 | Making short term '\n",
      "          'decisions  44:00 | Radical candor ',\n",
      "  'label': 'Feed Your Business Instead of Your Business Feeding You',\n",
      "  'score': 0.5058521032333374},\n",
      " {'data': 'We talk some Game of Thrones, (minor spoilers*) and the '\n",
      "          'controversial Arya Stark sex scene. Then we share some behind the '\n",
      "          'scenes stories of working in radio, before diving into a news '\n",
      "          'article out of Coachella, California on one despicable human being.',\n",
      "  'label': 'Season 2 - Episode 10 - April 24 2019',\n",
      "  'score': 0.5032269954681396}]\n"
     ]
    }
   ],
   "source": [
    "query = encode(\"news with edge.  talk about hard hitting facts where news is real.\")\n",
    "dataemb = tf.constant(emb,dtype=\"float32\")\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "lst = []\n",
    "n = 5\n",
    "for i in sorted_indices[:n]:\n",
    "    #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "    #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "    #print('')\n",
    "    #print('')\n",
    "    lst.append({\n",
    "        \"label\": loaded_data['embedding_labels'][i],\n",
    "        \"score\": scores[i],\n",
    "        \"data\":df[df[\"episode_name\"]==loaded_data['embedding_labels'][i]][\"episode_description\"].iloc[0]\n",
    "    })\n",
    "import pprint\n",
    "pprint.pprint(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93c4e0-f29e-4b8f-aec4-4c8f39738f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
