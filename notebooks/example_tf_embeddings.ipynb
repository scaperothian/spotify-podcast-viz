{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22ea4f7-3b6b-4554-aeda-ae85e7f96f0a",
   "metadata": {},
   "source": [
    "# Work on embeddings\n",
    "\n",
    "Objective: Take Podcast Descriptions and create embeddings from them.  Create an application that allows a user to enter their own text and use cosine similarity to find the most similar show descriptions (or episodes based on episode description).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "bcf087d1-9581-4d00-89e4-11765da5cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import tqdm\n",
    "import os\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "ea783ca8-cf7b-44f8-9908-f1c9eb5da55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90871, 18)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../metadata_with_episode_dates_and_category.tsv',sep='\\t')\n",
    "df['release_date'] = pd.to_datetime(df['release_date'], format='%Y-%m-%d').reset_index(drop=True)\n",
    "df = df[~df['release_date'].isna()]\n",
    "df = df[~df['category'].isna()]\n",
    "df = df[~df['show_description'].isna()]\n",
    "df = df[~df['show_name'].isna()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4599085b-7d8b-4677-ad3b-79cca4c5cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e88f36ae-6d09-4c25-adce-d0a3c97ad7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shows: 15857, Episodes: 90871\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shows: {len(show_descriptions)}, Episodes: {len(episode_descriptions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5dc47852-b743-48d4-8e03-eecc2c71dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = tf.cast(tf.tile(tf.expand_dims(attention_mask, -1), [1, 1, token_embeddings.shape[-1]]), tf.float32)\n",
    "    return tf.math.reduce_sum(token_embeddings * input_mask_expanded, 1) / tf.math.maximum(tf.math.reduce_sum(input_mask_expanded, 1), 1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = TFAutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6909a82a-209d-47e9-af7e-bba9e7ad3836",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Example from Huggingface\n",
    "https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "47cdf890-cc9f-47f6-9852-fddad6d7b19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.92, Query: Around 9 Million people live in London.\n",
      "Score: 0.49, Query: London is known for its financial district.\n"
     ]
    }
   ],
   "source": [
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)\n",
    "doc_emb = encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query_emb @ tf.transpose(doc_emb))[0].numpy().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(f\"Score: {score:.2f}, Query: {doc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ae4b7-20b2-4af6-8869-8327ceb7c495",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13beeb-a70e-4791-a9d5-e8df4a8ce7a5",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Performing Comparisons\n",
    "\n",
    "Benchmark embeddings comparison for all Show Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cf2e6f10-574b-41b1-b271-31895d07d047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35s to compare a worst cast dot product with 100000 rows.\n"
     ]
    }
   ],
   "source": [
    "randomquery_raw = \"Random Query\"\n",
    "random_query = encode(randomquery_raw)\n",
    "large_embed = tf.constant(np.random.random((100000,384)), dtype=tf.float32)\n",
    "start = time.time()\n",
    "scores = (random_query @ tf.transpose(large_embed))[0].numpy().tolist()\n",
    "end = time.time()\n",
    "print(f\"{end-start:.2f}s to compare a worst cast dot product with 100000 rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9251cb4-301c-4aa1-98b9-58be8a034820",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Making Show Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "4b04f5ae-79db-48db-a5d7-de5c6653d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "show_descriptions = list(df_filtered['show_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "28228ae9-8944-4839-aaee-753c5dc60990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08s: 1 Show Embedding(s)\n",
      "0.09s: 3 Show Embedding(s)\n",
      "0.12s: 10 Show Embedding(s)\n",
      "0.27s: 30 Show Embedding(s)\n",
      "1.10s: 100 Show Embedding(s)\n",
      "9.62s: 300 Show Embedding(s)\n"
     ]
    }
   ],
   "source": [
    "# Note: on show description embedding size of 1000, my macbook ran out of memory.\n",
    "t = []\n",
    "for i in [1, 3, 10, 30, 100, 300]:\n",
    "    start = time.time()\n",
    "    shows_embeddings = encode(show_descriptions[:i])\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print(f\"{end-start:.2f}s: {i} Show Embedding(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "66fc0b4b-bbbd-484e-a521-d608a129a5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Show Embeddings size: 30.  Maximizes Embeddings Per Second on my local machine\n"
     ]
    }
   ],
   "source": [
    "embedding_size_trials = np.array([1, 3, 10, 30, 100, 300])\n",
    "# take the embedding size trials and then take the argmax of the Embeddings Per Second Metric\n",
    "# which gives you the best index from embedding_size_trials.  this becomes the best_size.\n",
    "embeddings_per_sec = embedding_size_trials / np.array(t)\n",
    "best_index = np.argmax(embeddings_per_sec)\n",
    "best_size = embedding_size_trials[best_index]\n",
    "print(f\"Best Show Embeddings size: {best_size}.  Maximizes Embeddings Per Second on my local machine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e25ef85-00d2-4c0e-b06f-3bc8b19fbd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39 minutes to create all show embeddings.\n"
     ]
    }
   ],
   "source": [
    "# To attempt embeddings for each show...\n",
    "# create 30 embeddings at a time would take: \n",
    "print(f\"{df_filtered.shape[0] / embeddings_per_sec[best_index] / 60 :.2f} minutes to create all show embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401edc7-9a56-4fc9-ac60-81f4ab8f77b0",
   "metadata": {},
   "source": [
    "## Objective: Benchmark Making Episode Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9c776b4f-ad81-4403-ae57-68b1d132406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "episode_descriptions = list(df['episode_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3e80961b-ba11-4b1c-a661-39c8b098ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16s: 1 Episode Embedding(s)\n",
      "0.12s: 3 Episode Embedding(s)\n",
      "0.20s: 10 Episode Embedding(s)\n",
      "0.76s: 30 Episode Embedding(s)\n",
      "4.25s: 100 Episode Embedding(s)\n"
     ]
    }
   ],
   "source": [
    "# Note: on show description embedding size of 1000, my macbook ran out of memory.\n",
    "t = []\n",
    "for i in [1, 3, 10, 30, 100]:\n",
    "    start = time.time()\n",
    "    episode_embeddings = encode(episode_descriptions[:i])\n",
    "    end = time.time()\n",
    "    t.append(end-start)\n",
    "    print(f\"{end-start:.2f}s: {i} Episode Embedding(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "35e766e4-f609-4caf-b365-ee4175c9eddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Episode Embeddings size: 10.  Maximizes Embeddings Per Second on my local machine\n"
     ]
    }
   ],
   "source": [
    "embedding_size_trials = np.array([1, 3, 10, 30, 100])\n",
    "# take the embedding size trials and then take the argmax of the Embeddings Per Second Metric\n",
    "# which gives you the best index from embedding_size_trials.  this becomes the best_size.\n",
    "embeddings_per_sec = embedding_size_trials / np.array(t)\n",
    "best_index = np.argmax(embeddings_per_sec)\n",
    "best_size = embedding_size_trials[best_index]\n",
    "print(f\"Best Episode Embeddings size: {best_size}.  Maximizes Embeddings Per Second on my local machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "bc292021-b450-4dc6-ad21-ac63b9168335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.56 minutes to create all episode embeddings.\n"
     ]
    }
   ],
   "source": [
    "# To attempt embeddings for each show...\n",
    "# create 30 embeddings at a time would take: \n",
    "print(f\"{df.shape[0] / embeddings_per_sec[best_index] / 60 :.2f} minutes to create all episode embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68d1c6-0dc2-461f-8979-f4242a575a4d",
   "metadata": {},
   "source": [
    "# Test Code for Generating encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7bbd3bc5-4068-411f-8354-13b49ff1f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with text key (column: show_name) and text to embed (column: show_description) \n",
    "df_filtered = df.drop_duplicates(['show_name','show_description'])[['show_name','show_description']].reset_index(drop=True)\n",
    "\n",
    "# Create blocks of data and call encode\n",
    "# Save the data in a systematic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e16c77ef-05f9-475c-9f49-e432cff9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block size\n",
    "block_size = 30\n",
    "\n",
    "# Iterate over consecutive blocks of rows\n",
    "num_rows = len(df_filtered)\n",
    "start_index = 0\n",
    "counter = 0\n",
    "while start_index < num_rows:\n",
    "    end_index = start_index + block_size if start_index + block_size < num_rows else num_rows\n",
    "    subset_df = df.iloc[start_index:end_index]\n",
    "\n",
    "    # Apply the encode function to the current block\n",
    "    #encode_function(subset_df)\n",
    "    #save_data(subset_df\n",
    "    start_index = end_index\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f7cbcaef-08de-4b04-924e-5c8b649b8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data structure of embeddings being saved.\n",
    "embedding_size = 384\n",
    "block_size = 30\n",
    "num_blocks = 3\n",
    "num_padding = 5\n",
    "files = []\n",
    "for i in range(num_blocks):\n",
    "    # fake data\n",
    "    d = {\n",
    "        \"block\":i,\n",
    "        \"show_names\":['show1','show2','show3'] * 10,\n",
    "        \"show_desc_embeddings\": np.ones((block_size,embedding_size))*i\n",
    "    }\n",
    "    block_num = d['block']\n",
    "    filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "    files.append(filename)\n",
    "    # Save to disk\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6bca1076-b984-414a-889c-d80017afc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 384) 90\n"
     ]
    }
   ],
   "source": [
    "def extract_shows(filenames):\n",
    "    \"\"\"\n",
    "    input args:\n",
    "        files - list of files used to save the embeddings.\n",
    "    return args: \n",
    "        embedding_matrix - a numpy array of size (block_size x num_blocks,embedding_size) \n",
    "        list_of_shows - a list of strings of len (block_size x num_blocks)\n",
    "    \"\"\"\n",
    "    # This code takes in a list of files, then loads them, \n",
    "    # extracts the embeddings and concatenates them with the other embeddings.\n",
    "    list_of_tensors = []\n",
    "    list_of_shows = []\n",
    "    for file in filenames: \n",
    "        # Load from disk\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        list_of_tensors.append(loaded_data['show_desc_embeddings'])\n",
    "        list_of_shows.extend(loaded_data['show_names'])\n",
    "    \n",
    "    embedding_matrix = np.vstack(list_of_tensors)\n",
    "    return embedding_matrix, list_of_shows\n",
    "\n",
    "a,b= extract_shows(files)\n",
    "print(a.shape, len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d9621-054e-44a3-b2bf-3b518031a461",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f8f700ce-41bf-4879-a1fd-a64f4a603b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Embeddings in Chunks: 529it [02:37,  3.36it/s]                                             \n",
      "Combining Embeddings: 100%|███████████████████████████████████| 529/529 [00:00<00:00, 2765.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.60493803024292\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class EmbeddingGen:\n",
    "    def __init__(self, df, data_key, label_key, block_size=1, encoder=None):\n",
    "        self.data_frame = df\n",
    "        self.data_key = data_key\n",
    "        self.label_key = label_key\n",
    "        self.block_size = block_size\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.files = []\n",
    "        self.embedding_matrix = []\n",
    "        self.embedding_labels = []\n",
    "        # \n",
    "        self._saveEmbeddingChunks()\n",
    "        self._combineEmbeddings()\n",
    "\n",
    "    def load(self,file):\n",
    "        \"\"\"\n",
    "        Loading embeddings from file.\n",
    "        \"\"\"\n",
    "        with open(file, 'rb') as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "        self.embedding_matrix = loaded_data['embeddings']\n",
    "        self.embedding_labels = loaded_data['embedding_labels']\n",
    "    \n",
    "    def compare(self,query,n=5):\n",
    "        df = self.data_frame\n",
    "        dataemb = tf.constant(self.embedding_matrix,dtype=\"float32\")\n",
    "        #Compute dot score between query and all document embeddings\n",
    "        scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        lst = []\n",
    "        for i in sorted_indices[:n]:\n",
    "            #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "            #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "            #print('')\n",
    "            #print('')\n",
    "            lst.append({\n",
    "                \"label\": self.embedding_labels[i],\n",
    "                \"score\": scores[i],\n",
    "                \"data\":df[df[self.label_key]==self.embedding_labels[i]][self.data_key].iloc[0]\n",
    "            })\n",
    "        return lst\n",
    "    \n",
    "    def _saveEmbeddingChunks(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate over consecutive blocks of rows\n",
    "        num_rows = len(self.data_frame)\n",
    "        start_index = 0\n",
    "        block_counter = 0\n",
    "        est_total_iterations = num_rows // self.block_size\n",
    "        # Initialize tqdm with the total number of iterations\n",
    "        with tqdm.tqdm(total=est_total_iterations, desc=\"Saving Embeddings in Chunks\") as pbar:\n",
    "        # Start your while loop\n",
    "            while start_index < num_rows:\n",
    "                end_index = start_index + self.block_size if start_index + self.block_size < num_rows else num_rows\n",
    "                subset_df = self.data_frame.iloc[start_index:end_index]\n",
    "            \n",
    "                # Apply the encode function to the current block\n",
    "                emb = self.encoder(list(subset_df[self.data_key]))\n",
    "                emb_labels = list(subset_df[self.label_key])\n",
    "                self._saveChunk(emb,emb_labels,block_counter)\n",
    "                \n",
    "                start_index = end_index\n",
    "                block_counter += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "    def save(self):\n",
    "        self._saveChunk(self.embedding_matrix, self.embedding_labels, filename=\"final.pkl\")\n",
    "    \n",
    "    def _saveChunk(self, embeddings, embedding_labels, block_num=0, filename=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"block\":block_num,\n",
    "            \"embedding_labels\":embedding_labels,\n",
    "            \"embeddings\": embeddings\n",
    "        }\n",
    "        block_num = d['block']\n",
    "        if not filename: \n",
    "            filename = f\"{block_num:0{num_padding}}_data.pkl\"\n",
    "            self.files.append(filename)\n",
    "            \n",
    "        # Save to disk\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(d, f)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        # Explicit method for cleaning up resources\n",
    "        for file in self.files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                #print(f\"File {file} deleted successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file}: {e}\")\n",
    "                \n",
    "    def _combineEmbeddings(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # This code takes in a list of files, then loads them, \n",
    "        # extracts the embeddings and concatenates them with the other embeddings.\n",
    "        list_of_embeddings = []\n",
    "        list_of_labels = []\n",
    "        for file in tqdm.tqdm(self.files,desc=\"Combining Embeddings\"): \n",
    "            # Load from disk\n",
    "            with open(file, 'rb') as f:\n",
    "                loaded_data = pickle.load(f)\n",
    "            list_of_embeddings.append(loaded_data['embeddings'])\n",
    "            list_of_labels.extend(loaded_data['embedding_labels'])\n",
    "        \n",
    "        self.embedding_matrix = np.vstack(list_of_embeddings)\n",
    "        self.embedding_labels = list_of_labels\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "embgen = EmbeddingGen(df_filtered, data_key='show_description', label_key='show_name', block_size=30, encoder=encode)\n",
    "embgen.save()\n",
    "end = time.time()\n",
    "print(f\"{end-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "526fe767-3ac2-4978-9b79-2aabe39656b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 00000_data.pkl deleted successfully.\n",
      "File 00001_data.pkl deleted successfully.\n",
      "File 00002_data.pkl deleted successfully.\n",
      "File 00003_data.pkl deleted successfully.\n",
      "File 00004_data.pkl deleted successfully.\n",
      "File 00005_data.pkl deleted successfully.\n",
      "File 00006_data.pkl deleted successfully.\n",
      "File 00007_data.pkl deleted successfully.\n",
      "File 00008_data.pkl deleted successfully.\n",
      "File 00009_data.pkl deleted successfully.\n",
      "File 00010_data.pkl deleted successfully.\n",
      "File 00011_data.pkl deleted successfully.\n",
      "File 00012_data.pkl deleted successfully.\n",
      "File 00013_data.pkl deleted successfully.\n",
      "File 00014_data.pkl deleted successfully.\n",
      "File 00015_data.pkl deleted successfully.\n",
      "File 00016_data.pkl deleted successfully.\n",
      "File 00017_data.pkl deleted successfully.\n",
      "File 00018_data.pkl deleted successfully.\n",
      "File 00019_data.pkl deleted successfully.\n",
      "File 00020_data.pkl deleted successfully.\n",
      "File 00021_data.pkl deleted successfully.\n",
      "File 00022_data.pkl deleted successfully.\n",
      "File 00023_data.pkl deleted successfully.\n",
      "File 00024_data.pkl deleted successfully.\n",
      "File 00025_data.pkl deleted successfully.\n",
      "File 00026_data.pkl deleted successfully.\n",
      "File 00027_data.pkl deleted successfully.\n",
      "File 00028_data.pkl deleted successfully.\n",
      "File 00029_data.pkl deleted successfully.\n",
      "File 00030_data.pkl deleted successfully.\n",
      "File 00031_data.pkl deleted successfully.\n",
      "File 00032_data.pkl deleted successfully.\n",
      "File 00033_data.pkl deleted successfully.\n",
      "File 00034_data.pkl deleted successfully.\n",
      "File 00035_data.pkl deleted successfully.\n",
      "File 00036_data.pkl deleted successfully.\n",
      "File 00037_data.pkl deleted successfully.\n",
      "File 00038_data.pkl deleted successfully.\n",
      "File 00039_data.pkl deleted successfully.\n",
      "File 00040_data.pkl deleted successfully.\n",
      "File 00041_data.pkl deleted successfully.\n",
      "File 00042_data.pkl deleted successfully.\n",
      "File 00043_data.pkl deleted successfully.\n",
      "File 00044_data.pkl deleted successfully.\n",
      "File 00045_data.pkl deleted successfully.\n",
      "File 00046_data.pkl deleted successfully.\n",
      "File 00047_data.pkl deleted successfully.\n",
      "File 00048_data.pkl deleted successfully.\n",
      "File 00049_data.pkl deleted successfully.\n",
      "File 00050_data.pkl deleted successfully.\n",
      "File 00051_data.pkl deleted successfully.\n",
      "File 00052_data.pkl deleted successfully.\n",
      "File 00053_data.pkl deleted successfully.\n",
      "File 00054_data.pkl deleted successfully.\n",
      "File 00055_data.pkl deleted successfully.\n",
      "File 00056_data.pkl deleted successfully.\n",
      "File 00057_data.pkl deleted successfully.\n",
      "File 00058_data.pkl deleted successfully.\n",
      "File 00059_data.pkl deleted successfully.\n",
      "File 00060_data.pkl deleted successfully.\n",
      "File 00061_data.pkl deleted successfully.\n",
      "File 00062_data.pkl deleted successfully.\n",
      "File 00063_data.pkl deleted successfully.\n",
      "File 00064_data.pkl deleted successfully.\n",
      "File 00065_data.pkl deleted successfully.\n",
      "File 00066_data.pkl deleted successfully.\n",
      "File 00067_data.pkl deleted successfully.\n",
      "File 00068_data.pkl deleted successfully.\n",
      "File 00069_data.pkl deleted successfully.\n",
      "File 00070_data.pkl deleted successfully.\n",
      "File 00071_data.pkl deleted successfully.\n",
      "File 00072_data.pkl deleted successfully.\n",
      "File 00073_data.pkl deleted successfully.\n",
      "File 00074_data.pkl deleted successfully.\n",
      "File 00075_data.pkl deleted successfully.\n",
      "File 00076_data.pkl deleted successfully.\n",
      "File 00077_data.pkl deleted successfully.\n",
      "File 00078_data.pkl deleted successfully.\n",
      "File 00079_data.pkl deleted successfully.\n",
      "File 00080_data.pkl deleted successfully.\n",
      "File 00081_data.pkl deleted successfully.\n",
      "File 00082_data.pkl deleted successfully.\n",
      "File 00083_data.pkl deleted successfully.\n",
      "File 00084_data.pkl deleted successfully.\n",
      "File 00085_data.pkl deleted successfully.\n",
      "File 00086_data.pkl deleted successfully.\n",
      "File 00087_data.pkl deleted successfully.\n",
      "File 00088_data.pkl deleted successfully.\n",
      "File 00089_data.pkl deleted successfully.\n",
      "File 00090_data.pkl deleted successfully.\n",
      "File 00091_data.pkl deleted successfully.\n",
      "File 00092_data.pkl deleted successfully.\n",
      "File 00093_data.pkl deleted successfully.\n",
      "File 00094_data.pkl deleted successfully.\n",
      "File 00095_data.pkl deleted successfully.\n",
      "File 00096_data.pkl deleted successfully.\n",
      "File 00097_data.pkl deleted successfully.\n",
      "File 00098_data.pkl deleted successfully.\n",
      "File 00099_data.pkl deleted successfully.\n",
      "File 00100_data.pkl deleted successfully.\n",
      "File 00101_data.pkl deleted successfully.\n",
      "File 00102_data.pkl deleted successfully.\n",
      "File 00103_data.pkl deleted successfully.\n",
      "File 00104_data.pkl deleted successfully.\n",
      "File 00105_data.pkl deleted successfully.\n",
      "File 00106_data.pkl deleted successfully.\n",
      "File 00107_data.pkl deleted successfully.\n",
      "File 00108_data.pkl deleted successfully.\n",
      "File 00109_data.pkl deleted successfully.\n",
      "File 00110_data.pkl deleted successfully.\n",
      "File 00111_data.pkl deleted successfully.\n",
      "File 00112_data.pkl deleted successfully.\n",
      "File 00113_data.pkl deleted successfully.\n",
      "File 00114_data.pkl deleted successfully.\n",
      "File 00115_data.pkl deleted successfully.\n",
      "File 00116_data.pkl deleted successfully.\n",
      "File 00117_data.pkl deleted successfully.\n",
      "File 00118_data.pkl deleted successfully.\n",
      "File 00119_data.pkl deleted successfully.\n",
      "File 00120_data.pkl deleted successfully.\n",
      "File 00121_data.pkl deleted successfully.\n",
      "File 00122_data.pkl deleted successfully.\n",
      "File 00123_data.pkl deleted successfully.\n",
      "File 00124_data.pkl deleted successfully.\n",
      "File 00125_data.pkl deleted successfully.\n",
      "File 00126_data.pkl deleted successfully.\n",
      "File 00127_data.pkl deleted successfully.\n",
      "File 00128_data.pkl deleted successfully.\n",
      "File 00129_data.pkl deleted successfully.\n",
      "File 00130_data.pkl deleted successfully.\n",
      "File 00131_data.pkl deleted successfully.\n",
      "File 00132_data.pkl deleted successfully.\n",
      "File 00133_data.pkl deleted successfully.\n",
      "File 00134_data.pkl deleted successfully.\n",
      "File 00135_data.pkl deleted successfully.\n",
      "File 00136_data.pkl deleted successfully.\n",
      "File 00137_data.pkl deleted successfully.\n",
      "File 00138_data.pkl deleted successfully.\n",
      "File 00139_data.pkl deleted successfully.\n",
      "File 00140_data.pkl deleted successfully.\n",
      "File 00141_data.pkl deleted successfully.\n",
      "File 00142_data.pkl deleted successfully.\n",
      "File 00143_data.pkl deleted successfully.\n",
      "File 00144_data.pkl deleted successfully.\n",
      "File 00145_data.pkl deleted successfully.\n",
      "File 00146_data.pkl deleted successfully.\n",
      "File 00147_data.pkl deleted successfully.\n",
      "File 00148_data.pkl deleted successfully.\n",
      "File 00149_data.pkl deleted successfully.\n",
      "File 00150_data.pkl deleted successfully.\n",
      "File 00151_data.pkl deleted successfully.\n",
      "File 00152_data.pkl deleted successfully.\n",
      "File 00153_data.pkl deleted successfully.\n",
      "File 00154_data.pkl deleted successfully.\n",
      "File 00155_data.pkl deleted successfully.\n",
      "File 00156_data.pkl deleted successfully.\n",
      "File 00157_data.pkl deleted successfully.\n",
      "File 00158_data.pkl deleted successfully.\n",
      "File 00159_data.pkl deleted successfully.\n",
      "File 00160_data.pkl deleted successfully.\n",
      "File 00161_data.pkl deleted successfully.\n",
      "File 00162_data.pkl deleted successfully.\n",
      "File 00163_data.pkl deleted successfully.\n",
      "File 00164_data.pkl deleted successfully.\n",
      "File 00165_data.pkl deleted successfully.\n",
      "File 00166_data.pkl deleted successfully.\n",
      "File 00167_data.pkl deleted successfully.\n",
      "File 00168_data.pkl deleted successfully.\n",
      "File 00169_data.pkl deleted successfully.\n",
      "File 00170_data.pkl deleted successfully.\n",
      "File 00171_data.pkl deleted successfully.\n",
      "File 00172_data.pkl deleted successfully.\n",
      "File 00173_data.pkl deleted successfully.\n",
      "File 00174_data.pkl deleted successfully.\n",
      "File 00175_data.pkl deleted successfully.\n",
      "File 00176_data.pkl deleted successfully.\n",
      "File 00177_data.pkl deleted successfully.\n",
      "File 00178_data.pkl deleted successfully.\n",
      "File 00179_data.pkl deleted successfully.\n",
      "File 00180_data.pkl deleted successfully.\n",
      "File 00181_data.pkl deleted successfully.\n",
      "File 00182_data.pkl deleted successfully.\n",
      "File 00183_data.pkl deleted successfully.\n",
      "File 00184_data.pkl deleted successfully.\n",
      "File 00185_data.pkl deleted successfully.\n",
      "File 00186_data.pkl deleted successfully.\n",
      "File 00187_data.pkl deleted successfully.\n",
      "File 00188_data.pkl deleted successfully.\n",
      "File 00189_data.pkl deleted successfully.\n",
      "File 00190_data.pkl deleted successfully.\n",
      "File 00191_data.pkl deleted successfully.\n",
      "File 00192_data.pkl deleted successfully.\n",
      "File 00193_data.pkl deleted successfully.\n",
      "File 00194_data.pkl deleted successfully.\n",
      "File 00195_data.pkl deleted successfully.\n",
      "File 00196_data.pkl deleted successfully.\n",
      "File 00197_data.pkl deleted successfully.\n",
      "File 00198_data.pkl deleted successfully.\n",
      "File 00199_data.pkl deleted successfully.\n",
      "File 00200_data.pkl deleted successfully.\n",
      "File 00201_data.pkl deleted successfully.\n",
      "File 00202_data.pkl deleted successfully.\n",
      "File 00203_data.pkl deleted successfully.\n",
      "File 00204_data.pkl deleted successfully.\n",
      "File 00205_data.pkl deleted successfully.\n",
      "File 00206_data.pkl deleted successfully.\n",
      "File 00207_data.pkl deleted successfully.\n",
      "File 00208_data.pkl deleted successfully.\n",
      "File 00209_data.pkl deleted successfully.\n",
      "File 00210_data.pkl deleted successfully.\n",
      "File 00211_data.pkl deleted successfully.\n",
      "File 00212_data.pkl deleted successfully.\n",
      "File 00213_data.pkl deleted successfully.\n",
      "File 00214_data.pkl deleted successfully.\n",
      "File 00215_data.pkl deleted successfully.\n",
      "File 00216_data.pkl deleted successfully.\n",
      "File 00217_data.pkl deleted successfully.\n",
      "File 00218_data.pkl deleted successfully.\n",
      "File 00219_data.pkl deleted successfully.\n",
      "File 00220_data.pkl deleted successfully.\n",
      "File 00221_data.pkl deleted successfully.\n",
      "File 00222_data.pkl deleted successfully.\n",
      "File 00223_data.pkl deleted successfully.\n",
      "File 00224_data.pkl deleted successfully.\n",
      "File 00225_data.pkl deleted successfully.\n",
      "File 00226_data.pkl deleted successfully.\n",
      "File 00227_data.pkl deleted successfully.\n",
      "File 00228_data.pkl deleted successfully.\n",
      "File 00229_data.pkl deleted successfully.\n",
      "File 00230_data.pkl deleted successfully.\n",
      "File 00231_data.pkl deleted successfully.\n",
      "File 00232_data.pkl deleted successfully.\n",
      "File 00233_data.pkl deleted successfully.\n",
      "File 00234_data.pkl deleted successfully.\n",
      "File 00235_data.pkl deleted successfully.\n",
      "File 00236_data.pkl deleted successfully.\n",
      "File 00237_data.pkl deleted successfully.\n",
      "File 00238_data.pkl deleted successfully.\n",
      "File 00239_data.pkl deleted successfully.\n",
      "File 00240_data.pkl deleted successfully.\n",
      "File 00241_data.pkl deleted successfully.\n",
      "File 00242_data.pkl deleted successfully.\n",
      "File 00243_data.pkl deleted successfully.\n",
      "File 00244_data.pkl deleted successfully.\n",
      "File 00245_data.pkl deleted successfully.\n",
      "File 00246_data.pkl deleted successfully.\n",
      "File 00247_data.pkl deleted successfully.\n",
      "File 00248_data.pkl deleted successfully.\n",
      "File 00249_data.pkl deleted successfully.\n",
      "File 00250_data.pkl deleted successfully.\n",
      "File 00251_data.pkl deleted successfully.\n",
      "File 00252_data.pkl deleted successfully.\n",
      "File 00253_data.pkl deleted successfully.\n",
      "File 00254_data.pkl deleted successfully.\n",
      "File 00255_data.pkl deleted successfully.\n",
      "File 00256_data.pkl deleted successfully.\n",
      "File 00257_data.pkl deleted successfully.\n",
      "File 00258_data.pkl deleted successfully.\n",
      "File 00259_data.pkl deleted successfully.\n",
      "File 00260_data.pkl deleted successfully.\n",
      "File 00261_data.pkl deleted successfully.\n",
      "File 00262_data.pkl deleted successfully.\n",
      "File 00263_data.pkl deleted successfully.\n",
      "File 00264_data.pkl deleted successfully.\n",
      "File 00265_data.pkl deleted successfully.\n",
      "File 00266_data.pkl deleted successfully.\n",
      "File 00267_data.pkl deleted successfully.\n",
      "File 00268_data.pkl deleted successfully.\n",
      "File 00269_data.pkl deleted successfully.\n",
      "File 00270_data.pkl deleted successfully.\n",
      "File 00271_data.pkl deleted successfully.\n",
      "File 00272_data.pkl deleted successfully.\n",
      "File 00273_data.pkl deleted successfully.\n",
      "File 00274_data.pkl deleted successfully.\n",
      "File 00275_data.pkl deleted successfully.\n",
      "File 00276_data.pkl deleted successfully.\n",
      "File 00277_data.pkl deleted successfully.\n",
      "File 00278_data.pkl deleted successfully.\n",
      "File 00279_data.pkl deleted successfully.\n",
      "File 00280_data.pkl deleted successfully.\n",
      "File 00281_data.pkl deleted successfully.\n",
      "File 00282_data.pkl deleted successfully.\n",
      "File 00283_data.pkl deleted successfully.\n",
      "File 00284_data.pkl deleted successfully.\n",
      "File 00285_data.pkl deleted successfully.\n",
      "File 00286_data.pkl deleted successfully.\n",
      "File 00287_data.pkl deleted successfully.\n",
      "File 00288_data.pkl deleted successfully.\n",
      "File 00289_data.pkl deleted successfully.\n",
      "File 00290_data.pkl deleted successfully.\n",
      "File 00291_data.pkl deleted successfully.\n",
      "File 00292_data.pkl deleted successfully.\n",
      "File 00293_data.pkl deleted successfully.\n",
      "File 00294_data.pkl deleted successfully.\n",
      "File 00295_data.pkl deleted successfully.\n",
      "File 00296_data.pkl deleted successfully.\n",
      "File 00297_data.pkl deleted successfully.\n",
      "File 00298_data.pkl deleted successfully.\n",
      "File 00299_data.pkl deleted successfully.\n",
      "File 00300_data.pkl deleted successfully.\n",
      "File 00301_data.pkl deleted successfully.\n",
      "File 00302_data.pkl deleted successfully.\n",
      "File 00303_data.pkl deleted successfully.\n",
      "File 00304_data.pkl deleted successfully.\n",
      "File 00305_data.pkl deleted successfully.\n",
      "File 00306_data.pkl deleted successfully.\n",
      "File 00307_data.pkl deleted successfully.\n",
      "File 00308_data.pkl deleted successfully.\n",
      "File 00309_data.pkl deleted successfully.\n",
      "File 00310_data.pkl deleted successfully.\n",
      "File 00311_data.pkl deleted successfully.\n",
      "File 00312_data.pkl deleted successfully.\n",
      "File 00313_data.pkl deleted successfully.\n",
      "File 00314_data.pkl deleted successfully.\n",
      "File 00315_data.pkl deleted successfully.\n",
      "File 00316_data.pkl deleted successfully.\n",
      "File 00317_data.pkl deleted successfully.\n",
      "File 00318_data.pkl deleted successfully.\n",
      "File 00319_data.pkl deleted successfully.\n",
      "File 00320_data.pkl deleted successfully.\n",
      "File 00321_data.pkl deleted successfully.\n",
      "File 00322_data.pkl deleted successfully.\n",
      "File 00323_data.pkl deleted successfully.\n",
      "File 00324_data.pkl deleted successfully.\n",
      "File 00325_data.pkl deleted successfully.\n",
      "File 00326_data.pkl deleted successfully.\n",
      "File 00327_data.pkl deleted successfully.\n",
      "File 00328_data.pkl deleted successfully.\n",
      "File 00329_data.pkl deleted successfully.\n",
      "File 00330_data.pkl deleted successfully.\n",
      "File 00331_data.pkl deleted successfully.\n",
      "File 00332_data.pkl deleted successfully.\n",
      "File 00333_data.pkl deleted successfully.\n",
      "File 00334_data.pkl deleted successfully.\n",
      "File 00335_data.pkl deleted successfully.\n",
      "File 00336_data.pkl deleted successfully.\n",
      "File 00337_data.pkl deleted successfully.\n",
      "File 00338_data.pkl deleted successfully.\n",
      "File 00339_data.pkl deleted successfully.\n",
      "File 00340_data.pkl deleted successfully.\n",
      "File 00341_data.pkl deleted successfully.\n",
      "File 00342_data.pkl deleted successfully.\n",
      "File 00343_data.pkl deleted successfully.\n",
      "File 00344_data.pkl deleted successfully.\n",
      "File 00345_data.pkl deleted successfully.\n",
      "File 00346_data.pkl deleted successfully.\n",
      "File 00347_data.pkl deleted successfully.\n",
      "File 00348_data.pkl deleted successfully.\n",
      "File 00349_data.pkl deleted successfully.\n",
      "File 00350_data.pkl deleted successfully.\n",
      "File 00351_data.pkl deleted successfully.\n",
      "File 00352_data.pkl deleted successfully.\n",
      "File 00353_data.pkl deleted successfully.\n",
      "File 00354_data.pkl deleted successfully.\n",
      "File 00355_data.pkl deleted successfully.\n",
      "File 00356_data.pkl deleted successfully.\n",
      "File 00357_data.pkl deleted successfully.\n",
      "File 00358_data.pkl deleted successfully.\n",
      "File 00359_data.pkl deleted successfully.\n",
      "File 00360_data.pkl deleted successfully.\n",
      "File 00361_data.pkl deleted successfully.\n",
      "File 00362_data.pkl deleted successfully.\n",
      "File 00363_data.pkl deleted successfully.\n",
      "File 00364_data.pkl deleted successfully.\n",
      "File 00365_data.pkl deleted successfully.\n",
      "File 00366_data.pkl deleted successfully.\n",
      "File 00367_data.pkl deleted successfully.\n",
      "File 00368_data.pkl deleted successfully.\n",
      "File 00369_data.pkl deleted successfully.\n",
      "File 00370_data.pkl deleted successfully.\n",
      "File 00371_data.pkl deleted successfully.\n",
      "File 00372_data.pkl deleted successfully.\n",
      "File 00373_data.pkl deleted successfully.\n",
      "File 00374_data.pkl deleted successfully.\n",
      "File 00375_data.pkl deleted successfully.\n",
      "File 00376_data.pkl deleted successfully.\n",
      "File 00377_data.pkl deleted successfully.\n",
      "File 00378_data.pkl deleted successfully.\n",
      "File 00379_data.pkl deleted successfully.\n",
      "File 00380_data.pkl deleted successfully.\n",
      "File 00381_data.pkl deleted successfully.\n",
      "File 00382_data.pkl deleted successfully.\n",
      "File 00383_data.pkl deleted successfully.\n",
      "File 00384_data.pkl deleted successfully.\n",
      "File 00385_data.pkl deleted successfully.\n",
      "File 00386_data.pkl deleted successfully.\n",
      "File 00387_data.pkl deleted successfully.\n",
      "File 00388_data.pkl deleted successfully.\n",
      "File 00389_data.pkl deleted successfully.\n",
      "File 00390_data.pkl deleted successfully.\n",
      "File 00391_data.pkl deleted successfully.\n",
      "File 00392_data.pkl deleted successfully.\n",
      "File 00393_data.pkl deleted successfully.\n",
      "File 00394_data.pkl deleted successfully.\n",
      "File 00395_data.pkl deleted successfully.\n",
      "File 00396_data.pkl deleted successfully.\n",
      "File 00397_data.pkl deleted successfully.\n",
      "File 00398_data.pkl deleted successfully.\n",
      "File 00399_data.pkl deleted successfully.\n",
      "File 00400_data.pkl deleted successfully.\n",
      "File 00401_data.pkl deleted successfully.\n",
      "File 00402_data.pkl deleted successfully.\n",
      "File 00403_data.pkl deleted successfully.\n",
      "File 00404_data.pkl deleted successfully.\n",
      "File 00405_data.pkl deleted successfully.\n",
      "File 00406_data.pkl deleted successfully.\n",
      "File 00407_data.pkl deleted successfully.\n",
      "File 00408_data.pkl deleted successfully.\n",
      "File 00409_data.pkl deleted successfully.\n",
      "File 00410_data.pkl deleted successfully.\n",
      "File 00411_data.pkl deleted successfully.\n",
      "File 00412_data.pkl deleted successfully.\n",
      "File 00413_data.pkl deleted successfully.\n",
      "File 00414_data.pkl deleted successfully.\n",
      "File 00415_data.pkl deleted successfully.\n",
      "File 00416_data.pkl deleted successfully.\n",
      "File 00417_data.pkl deleted successfully.\n",
      "File 00418_data.pkl deleted successfully.\n",
      "File 00419_data.pkl deleted successfully.\n",
      "File 00420_data.pkl deleted successfully.\n",
      "File 00421_data.pkl deleted successfully.\n",
      "File 00422_data.pkl deleted successfully.\n",
      "File 00423_data.pkl deleted successfully.\n",
      "File 00424_data.pkl deleted successfully.\n",
      "File 00425_data.pkl deleted successfully.\n",
      "File 00426_data.pkl deleted successfully.\n",
      "File 00427_data.pkl deleted successfully.\n",
      "File 00428_data.pkl deleted successfully.\n",
      "File 00429_data.pkl deleted successfully.\n",
      "File 00430_data.pkl deleted successfully.\n",
      "File 00431_data.pkl deleted successfully.\n",
      "File 00432_data.pkl deleted successfully.\n",
      "File 00433_data.pkl deleted successfully.\n",
      "File 00434_data.pkl deleted successfully.\n",
      "File 00435_data.pkl deleted successfully.\n",
      "File 00436_data.pkl deleted successfully.\n",
      "File 00437_data.pkl deleted successfully.\n",
      "File 00438_data.pkl deleted successfully.\n",
      "File 00439_data.pkl deleted successfully.\n",
      "File 00440_data.pkl deleted successfully.\n",
      "File 00441_data.pkl deleted successfully.\n",
      "File 00442_data.pkl deleted successfully.\n",
      "File 00443_data.pkl deleted successfully.\n",
      "File 00444_data.pkl deleted successfully.\n",
      "File 00445_data.pkl deleted successfully.\n",
      "File 00446_data.pkl deleted successfully.\n",
      "File 00447_data.pkl deleted successfully.\n",
      "File 00448_data.pkl deleted successfully.\n",
      "File 00449_data.pkl deleted successfully.\n",
      "File 00450_data.pkl deleted successfully.\n",
      "File 00451_data.pkl deleted successfully.\n",
      "File 00452_data.pkl deleted successfully.\n",
      "File 00453_data.pkl deleted successfully.\n",
      "File 00454_data.pkl deleted successfully.\n",
      "File 00455_data.pkl deleted successfully.\n",
      "File 00456_data.pkl deleted successfully.\n",
      "File 00457_data.pkl deleted successfully.\n",
      "File 00458_data.pkl deleted successfully.\n",
      "File 00459_data.pkl deleted successfully.\n",
      "File 00460_data.pkl deleted successfully.\n",
      "File 00461_data.pkl deleted successfully.\n",
      "File 00462_data.pkl deleted successfully.\n",
      "File 00463_data.pkl deleted successfully.\n",
      "File 00464_data.pkl deleted successfully.\n",
      "File 00465_data.pkl deleted successfully.\n",
      "File 00466_data.pkl deleted successfully.\n",
      "File 00467_data.pkl deleted successfully.\n",
      "File 00468_data.pkl deleted successfully.\n",
      "File 00469_data.pkl deleted successfully.\n",
      "File 00470_data.pkl deleted successfully.\n",
      "File 00471_data.pkl deleted successfully.\n",
      "File 00472_data.pkl deleted successfully.\n",
      "File 00473_data.pkl deleted successfully.\n",
      "File 00474_data.pkl deleted successfully.\n",
      "File 00475_data.pkl deleted successfully.\n",
      "File 00476_data.pkl deleted successfully.\n",
      "File 00477_data.pkl deleted successfully.\n",
      "File 00478_data.pkl deleted successfully.\n",
      "File 00479_data.pkl deleted successfully.\n",
      "File 00480_data.pkl deleted successfully.\n",
      "File 00481_data.pkl deleted successfully.\n",
      "File 00482_data.pkl deleted successfully.\n",
      "File 00483_data.pkl deleted successfully.\n",
      "File 00484_data.pkl deleted successfully.\n",
      "File 00485_data.pkl deleted successfully.\n",
      "File 00486_data.pkl deleted successfully.\n",
      "File 00487_data.pkl deleted successfully.\n",
      "File 00488_data.pkl deleted successfully.\n",
      "File 00489_data.pkl deleted successfully.\n",
      "File 00490_data.pkl deleted successfully.\n",
      "File 00491_data.pkl deleted successfully.\n",
      "File 00492_data.pkl deleted successfully.\n",
      "File 00493_data.pkl deleted successfully.\n",
      "File 00494_data.pkl deleted successfully.\n",
      "File 00495_data.pkl deleted successfully.\n",
      "File 00496_data.pkl deleted successfully.\n",
      "File 00497_data.pkl deleted successfully.\n",
      "File 00498_data.pkl deleted successfully.\n",
      "File 00499_data.pkl deleted successfully.\n",
      "File 00500_data.pkl deleted successfully.\n",
      "File 00501_data.pkl deleted successfully.\n",
      "File 00502_data.pkl deleted successfully.\n",
      "File 00503_data.pkl deleted successfully.\n",
      "File 00504_data.pkl deleted successfully.\n",
      "File 00505_data.pkl deleted successfully.\n",
      "File 00506_data.pkl deleted successfully.\n",
      "File 00507_data.pkl deleted successfully.\n",
      "File 00508_data.pkl deleted successfully.\n",
      "File 00509_data.pkl deleted successfully.\n",
      "File 00510_data.pkl deleted successfully.\n",
      "File 00511_data.pkl deleted successfully.\n",
      "File 00512_data.pkl deleted successfully.\n",
      "File 00513_data.pkl deleted successfully.\n",
      "File 00514_data.pkl deleted successfully.\n",
      "File 00515_data.pkl deleted successfully.\n",
      "File 00516_data.pkl deleted successfully.\n",
      "File 00517_data.pkl deleted successfully.\n",
      "File 00518_data.pkl deleted successfully.\n",
      "File 00519_data.pkl deleted successfully.\n",
      "File 00520_data.pkl deleted successfully.\n",
      "File 00521_data.pkl deleted successfully.\n",
      "File 00522_data.pkl deleted successfully.\n",
      "File 00523_data.pkl deleted successfully.\n",
      "File 00524_data.pkl deleted successfully.\n",
      "File 00525_data.pkl deleted successfully.\n",
      "File 00526_data.pkl deleted successfully.\n",
      "File 00527_data.pkl deleted successfully.\n",
      "File 00528_data.pkl deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "embgen.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "73d5a3fe-0f59-43a0-b056-1830f5f5f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': '2 Private Cannabis Investors share thoughts, analysis and opinions '\n",
      "          'on the ups and downs of the rapidly changing Cannabis Investing '\n",
      "          'landscape. For Investors By Investors',\n",
      "  'label': 'Cannabis Investing Network',\n",
      "  'score': 0.8425226211547852},\n",
      " {'data': 'As a business accelerator and investor in ancillary cannabis '\n",
      "          'companies, we’ve helped launch 90+ companies and have made 110+ '\n",
      "          'investments. And along the way, we’ve seen a lot! Now we’re sharing '\n",
      "          'everything you need to know about starting up and investing in the '\n",
      "          'legal cannabis industry. We post every other week! Learn more about '\n",
      "          'what we do by visiting us at www.canopyboulder.com. ',\n",
      "  'label': 'CanopyBoulder Cannabis Business Podcast',\n",
      "  'score': 0.7082740664482117},\n",
      " {'data': 'Focused on cannabis growing and the culture, technology, and more '\n",
      "          'for cannabis growers, users, and curious folks alike!',\n",
      "  'label': 'Greenstalk Talks',\n",
      "  'score': 0.6654050350189209},\n",
      " {'data': 'Promoting the hard working individuals of our amazing cannabis '\n",
      "          'community. We will stand together 🙏',\n",
      "  'label': 'Mount Cooked ',\n",
      "  'score': 0.6576687097549438},\n",
      " {'data': 'Cannabis 101 : Professional Interviews : Life in the Cannabis '\n",
      "          'industry :',\n",
      "  'label': 'Growing Weed Is for Dummies!',\n",
      "  'score': 0.6046208143234253}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(embgen.compare(query0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "7a005470-d6af-422c-8d52-4b952618b3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Frillz Podcast with Yipes & Matrix',\n",
       " 'Ayodya Talk',\n",
       " 'Coca-cola',\n",
       " 'The Motivational Dude Podcast',\n",
       " 'Uwu',\n",
       " 'The Phoenix Project Podcast',\n",
       " 'Is it in yet? A sex podcast',\n",
       " 'Star Wars Sessions',\n",
       " 'The Culture Project Podcast',\n",
       " 'Women That Wait (WTW)']"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embgen.embedding_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2f923dbb-ea41-4756-a435-0a97d18b299e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannabis Investing Network: 2 Private Cannabis Investors share thoughts, analysis and opinions on the ups and downs of the rapidly changing Cannabis Investing landscape. For Investors By Investors\n"
     ]
    }
   ],
   "source": [
    "REF_INDEX = 505\n",
    "desc = df_filtered['show_description'].iloc[REF_INDEX]\n",
    "name = df_filtered['show_name'].iloc[REF_INDEX]\n",
    "print(f\"{name}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "0aae9c0d-0ff4-4605-929a-3a1b9bc274bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query0_raw = \"Private Cannabis Investors share thoughts.\"\n",
    "query0 = encode(query0_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "3848b494-826c-4b6c-a642-19662e9fa9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100,   7, 154, 143,  32,  80,  44, 142,  13, 119, 130,  10,  85,\n",
       "       102, 109,  54, 104, 147, 123,  68,  82, 126,  84,  41,  93,  98,\n",
       "       118,  15, 146,  90,  55,   4, 134,  30, 149, 108, 141,  42,  67,\n",
       "       111, 144,  23,  25, 128,   3, 124,  22,  53,  18, 138, 150,  59,\n",
       "        56,  65,  12,  75,   9,  36, 115,  89,  95, 114,   8,  52,  29,\n",
       "       151, 113,  81, 135, 127,  49, 133,  39,   1,  63, 101,  47, 148,\n",
       "        14, 139,  57, 152,  37,  46,  19,  35,  77,  27, 122,  96,  69,\n",
       "       140,  83, 137, 153,  20, 103,  72,   5, 131,  34,  51, 110, 116,\n",
       "        48,  99,  94,  97,  38,  40,   6, 125,  60,  66,  88, 106, 121,\n",
       "        16, 129, 107,  33,  91,  71,  61,  45,  31,  24,  79,  87,  76,\n",
       "        62,  73,  43,  74,  58, 112,  64,  70,  21, 132, 120,   0,  26,\n",
       "        17, 117, 136,  86, 145,  92,  11,   2, 105,  50,  78,  28])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataemb = tf.constant(embgen.embedding_matrix,dtype=\"float32\")\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = (query0 @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "#display(scores)\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "06887eba-8a2a-4c12-a414-7863f6b0b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insights with Joe Pane: 0.79\n",
      "This podcast is dedicated to those of us on a journey from ambition to meaning. I share with you the experiences of 1000's of people I have had the honor of coaching, training and leading over the last decade and a half, who have each embarked on this journey. This podcast is about redefining success. Ultimately, success is about the value we have been to someone else. This kind of success flavors our ultimate legacy, which is the contribution we have made to the live's of others. Thank you and I look forward to sharing all I can about this beautiful journey. \n",
      "\n",
      "\n",
      "The Good Sign : 0.66\n",
      "Let’s be real and honest! Life can be challenging and stressful! I am a mom, a teacher, a life coach and a motivational speaker. Meeting so many people from so many places has made me realize that we are all on a similar quest for happiness and inspiration. This podcast will be uplifting, honest and damn funny! Join me each Monday night as me and my guests impart attainable goals and strategies that will uplift you and make you smile. \n",
      "\n",
      "\n",
      "United in Motherhood by Zoe Young : 0.58\n",
      "The podcast for every woman, for every Mama. The #unitedinmotherhood podcast is a safe space. It's where I share with you, the stories of incredible women. Women that have us all seeing the world in a different way. Sharing their stories to empower, drive connection and uplift us all to feel United In Motherhood. Cause let's face it, together we are stronger! \n",
      "\n",
      "\n",
      "The Lyndsey Morrison Podcast: 0.57\n",
      "This Podcast is for Group Fitness Instructors. I Share Insight and Experiences From My Own Group Fitness Journey and Interview Fitness Leaders That Are Impacting The Fitness Arena.  \n",
      "\n",
      "\n",
      "Always Be Raising Podcast - by G&H Ventures: 0.56\n",
      "Always Be Raising Podcast is a series by G&H Ventures, an early-stage tech investor in Vietnam. We will interview top venture capitalists and founders in Vietnam on startup fundraising, operations, tech trends, and the future.   Join us and share your story!   Contact me at: louis.nguyen@ghventures.vc\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_indices[:5]:\n",
    "    print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "    print(df_filtered[df_filtered['show_name']==embgen.embedding_labels[i]].show_description.iloc[0])\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "b46b3e15-5002-451e-94e9-5171a337535f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This podcast is dedicated to those of us on a journey from ambition '\n",
      "          \"to meaning. I share with you the experiences of 1000's of people I \"\n",
      "          'have had the honor of coaching, training and leading over the last '\n",
      "          'decade and a half, who have each embarked on this journey. This '\n",
      "          'podcast is about redefining success. Ultimately, success is about '\n",
      "          'the value we have been to someone else. This kind of success '\n",
      "          'flavors our ultimate legacy, which is the contribution we have made '\n",
      "          \"to the live's of others. Thank you and I look forward to sharing \"\n",
      "          'all I can about this beautiful journey. ',\n",
      "  'label': 'Insights with Joe Pane',\n",
      "  'score': 0.7919762134552002},\n",
      " {'data': 'Let’s be real and honest! Life can be challenging and stressful! I '\n",
      "          'am a mom, a teacher, a life coach and a motivational speaker. '\n",
      "          'Meeting so many people from so many places has made me realize that '\n",
      "          'we are all on a similar quest for happiness and inspiration. This '\n",
      "          'podcast will be uplifting, honest and damn funny! Join me each '\n",
      "          'Monday night as me and my guests impart attainable goals and '\n",
      "          'strategies that will uplift you and make you smile. ',\n",
      "  'label': 'The Good Sign ',\n",
      "  'score': 0.6577978134155273},\n",
      " {'data': 'The podcast for every woman, for every Mama. The '\n",
      "          \"#unitedinmotherhood podcast is a safe space. It's where I share \"\n",
      "          'with you, the stories of incredible women. Women that have us all '\n",
      "          'seeing the world in a different way. Sharing their stories to '\n",
      "          'empower, drive connection and uplift us all to feel United In '\n",
      "          \"Motherhood. Cause let's face it, together we are stronger! \",\n",
      "  'label': 'United in Motherhood by Zoe Young ',\n",
      "  'score': 0.5826876759529114},\n",
      " {'data': 'This Podcast is for Group Fitness Instructors. I Share Insight and '\n",
      "          'Experiences From My Own Group Fitness Journey and Interview Fitness '\n",
      "          'Leaders That Are Impacting The Fitness Arena.  ',\n",
      "  'label': 'The Lyndsey Morrison Podcast',\n",
      "  'score': 0.5743718147277832},\n",
      " {'data': 'Always Be Raising Podcast is a series by G&H Ventures, an '\n",
      "          'early-stage tech investor in Vietnam. We will interview top venture '\n",
      "          'capitalists and founders in Vietnam on startup fundraising, '\n",
      "          'operations, tech trends, and the future.   Join us and share your '\n",
      "          'story!   Contact me at: louis.nguyen@ghventures.vc',\n",
      "  'label': 'Always Be Raising Podcast - by G&H Ventures',\n",
      "  'score': 0.5638134479522705}]\n"
     ]
    }
   ],
   "source": [
    "def find_top_n(objs,query,n=5):\n",
    "    df = objs.data_frame\n",
    "    dataemb = tf.constant(objs.embedding_matrix,dtype=\"float32\")\n",
    "    #Compute dot score between query and all document embeddings\n",
    "    scores = (query @ tf.transpose(dataemb))[0].numpy().tolist()\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    lst = []\n",
    "    for i in sorted_indices[:n]:\n",
    "        #print(f\"{embgen.embedding_labels[i]}: {scores[i]:.2f}\")\n",
    "        #print(df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0])\n",
    "        #print('')\n",
    "        #print('')\n",
    "        lst.append({\n",
    "            \"label\": embgen.embedding_labels[i],\n",
    "            \"score\": scores[i],\n",
    "            \"data\":df[df[objs.label_key]==embgen.embedding_labels[i]][objs.data_key].iloc[0]\n",
    "        })\n",
    "    return lst\n",
    "import pprint\n",
    "pprint.pprint(find_top_n(embgen, query0, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6d77be43-0bc5-43a1-a575-ecf9a65d7b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'data': 'This podcast is dedicated to those of us on a journey from ambition '\n",
      "          \"to meaning. I share with you the experiences of 1000's of people I \"\n",
      "          'have had the honor of coaching, training and leading over the last '\n",
      "          'decade and a half, who have each embarked on this journey. This '\n",
      "          'podcast is about redefining success. Ultimately, success is about '\n",
      "          'the value we have been to someone else. This kind of success '\n",
      "          'flavors our ultimate legacy, which is the contribution we have made '\n",
      "          \"to the live's of others. Thank you and I look forward to sharing \"\n",
      "          'all I can about this beautiful journey. ',\n",
      "  'label': 'Insights with Joe Pane',\n",
      "  'score': 0.7919762134552002},\n",
      " {'data': 'Let’s be real and honest! Life can be challenging and stressful! I '\n",
      "          'am a mom, a teacher, a life coach and a motivational speaker. '\n",
      "          'Meeting so many people from so many places has made me realize that '\n",
      "          'we are all on a similar quest for happiness and inspiration. This '\n",
      "          'podcast will be uplifting, honest and damn funny! Join me each '\n",
      "          'Monday night as me and my guests impart attainable goals and '\n",
      "          'strategies that will uplift you and make you smile. ',\n",
      "  'label': 'The Good Sign ',\n",
      "  'score': 0.6577978134155273},\n",
      " {'data': 'The podcast for every woman, for every Mama. The '\n",
      "          \"#unitedinmotherhood podcast is a safe space. It's where I share \"\n",
      "          'with you, the stories of incredible women. Women that have us all '\n",
      "          'seeing the world in a different way. Sharing their stories to '\n",
      "          'empower, drive connection and uplift us all to feel United In '\n",
      "          \"Motherhood. Cause let's face it, together we are stronger! \",\n",
      "  'label': 'United in Motherhood by Zoe Young ',\n",
      "  'score': 0.5826876759529114},\n",
      " {'data': 'This Podcast is for Group Fitness Instructors. I Share Insight and '\n",
      "          'Experiences From My Own Group Fitness Journey and Interview Fitness '\n",
      "          'Leaders That Are Impacting The Fitness Arena.  ',\n",
      "  'label': 'The Lyndsey Morrison Podcast',\n",
      "  'score': 0.5743718147277832},\n",
      " {'data': 'Always Be Raising Podcast is a series by G&H Ventures, an '\n",
      "          'early-stage tech investor in Vietnam. We will interview top venture '\n",
      "          'capitalists and founders in Vietnam on startup fundraising, '\n",
      "          'operations, tech trends, and the future.   Join us and share your '\n",
      "          'story!   Contact me at: louis.nguyen@ghventures.vc',\n",
      "  'label': 'Always Be Raising Podcast - by G&H Ventures',\n",
      "  'score': 0.5638134479522705}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(embgen.compare(query0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
